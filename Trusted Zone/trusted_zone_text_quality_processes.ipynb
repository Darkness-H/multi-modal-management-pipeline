{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dc763fb-323c-4ea3-88ab-679b0f4547be",
   "metadata": {},
   "source": [
    "## Quality Processes for Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1761dc81-08af-4c98-bc2b-dd46a068c54e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install langdetect googletrans deep_translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8d07226-0c4f-46cf-8620-3d13019ee2b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Importing useful dependencies\n",
    "import re\n",
    "import unicodedata\n",
    "import io\n",
    "import os\n",
    "import boto3\n",
    "import chardet\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "from langdetect import detect\n",
    "from googletrans import Translator\n",
    "import asyncio\n",
    "from deep_translator import GoogleTranslator\n",
    "# set tokenizer with openAI standard token\n",
    "from typing import List, Dict\n",
    "from ftfy import fix_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "def1c220-1a48-4f4e-bf08-8dabcf4c488f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup S3 client for MinIO (MinIO implements Amazon S3 API)\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    endpoint_url=\"http://127.0.0.1:9000\", # MinIO API endpoint\n",
    "    aws_access_key_id=\"minioadmin\", # User name\n",
    "    aws_secret_access_key=\"minioadmin\", # Password\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66f0316a-3375-40e7-9102-e86b69124317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Quality checks ----\n",
    "def check_text_quality(body: bytes, key: str):\n",
    "    \"\"\"Return a simple dict of basic quality stats for one file.\"\"\"\n",
    "    if not body:\n",
    "        return {\"key\": key, \"empty\": True}\n",
    "    \n",
    "    # detect encoding and decode safely\n",
    "    guess = chardet.detect(body)\n",
    "    enc = guess.get(\"encoding\") or \"utf-8\"\n",
    "    text = body.decode(enc, errors=\"replace\")\n",
    "\n",
    "    # check printable ratio (avoid binary garbage)\n",
    "    printable_ratio = sum(c.isprintable() or c.isspace() for c in text) / max(1, len(text))\n",
    "    origin_lang = detect(text)\n",
    "    # --- Paragraph token stats ---\n",
    "    paragraphs = [p.strip() for p in text.split(\"\\n\\n\") if p.strip()]\n",
    "\n",
    "    paragraph_tokens = [len(p) for p in paragraphs] if paragraphs else []\n",
    "    \n",
    "    avg_tokens_per_paragraph = (\n",
    "        sum(paragraph_tokens) / len(paragraph_tokens) if paragraph_tokens else 0\n",
    "    )\n",
    "    max_tokens_paragraph = max(paragraph_tokens) if paragraph_tokens else 0\n",
    "    min_tokens_paragraph = min(paragraph_tokens) if paragraph_tokens else 0\n",
    "    # basic stats\n",
    "    return {\n",
    "        \"key\": key,\n",
    "        \"size_bytes\": len(body),\n",
    "        \"encoding\": enc,\n",
    "        \"empty\": not bool(text.strip()),\n",
    "        \"too_short\": len(text.strip()) < 20,\n",
    "        \"low_printable_ratio\": printable_ratio < 0.9,\n",
    "        \"lines\": len(text.splitlines()),\n",
    "        \"avg_tokens_per_paragraph\": round(avg_tokens_per_paragraph, 2),\n",
    "        \"max_tokens_paragraph\": max_tokens_paragraph,\n",
    "        \"min_tokens_paragraph\": min_tokens_paragraph,\n",
    "        \"paragraph_token_list\": paragraph_tokens,  # you can drop this if you don’t need full list\n",
    "        \"origin_language\": origin_lang,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f1e5910-5edf-4b0d-9706-911d91a9a043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Run checks on all txt files ----\n",
    "def extract_datas(bucket,prefix=\"\"):\n",
    "    results = []\n",
    "    paginator = s3.get_paginator(\"list_objects_v2\")\n",
    "    for page in paginator.paginate(Bucket=bucket, Prefix=prefix):\n",
    "        for obj in page.get(\"Contents\", []):\n",
    "            \n",
    "            key = obj[\"Key\"]\n",
    "\n",
    "            if obj['Size'] == 0 and key.endswith(\"/\"): # skip the folder itself\n",
    "                continue\n",
    "            # Download the text\n",
    "            resp = s3.get_object(Bucket=bucket, Key=key)\n",
    "            body = resp[\"Body\"].read()\n",
    "            stats = check_text_quality(body, key)\n",
    "            results.append(stats)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f2054ca-2f79-4ba8-9650-bd7fc09a0a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = extract_datas(bucket = \"trusted-zone\", prefix = \"texts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86b9d6a3-cd3a-439d-82be-e140759a4504",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>size_bytes</th>\n",
       "      <th>encoding</th>\n",
       "      <th>empty</th>\n",
       "      <th>too_short</th>\n",
       "      <th>low_printable_ratio</th>\n",
       "      <th>lines</th>\n",
       "      <th>avg_tokens_per_paragraph</th>\n",
       "      <th>max_tokens_paragraph</th>\n",
       "      <th>min_tokens_paragraph</th>\n",
       "      <th>paragraph_token_list</th>\n",
       "      <th>origin_language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>texts/text_1760786400687.txt</td>\n",
       "      <td>1043</td>\n",
       "      <td>ascii</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>1043.0</td>\n",
       "      <td>1043</td>\n",
       "      <td>1043</td>\n",
       "      <td>[1043]</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>texts/text_1760786400752.txt</td>\n",
       "      <td>1373</td>\n",
       "      <td>utf-8</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>1369.0</td>\n",
       "      <td>1369</td>\n",
       "      <td>1369</td>\n",
       "      <td>[1369]</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>texts/text_1760786400827.txt</td>\n",
       "      <td>734</td>\n",
       "      <td>utf-8</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>729.0</td>\n",
       "      <td>729</td>\n",
       "      <td>729</td>\n",
       "      <td>[729]</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>texts/text_1760786400902.txt</td>\n",
       "      <td>1072</td>\n",
       "      <td>ascii</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>1072.0</td>\n",
       "      <td>1072</td>\n",
       "      <td>1072</td>\n",
       "      <td>[1072]</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>texts/text_1760786400988.txt</td>\n",
       "      <td>1260</td>\n",
       "      <td>ascii</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>1260.0</td>\n",
       "      <td>1260</td>\n",
       "      <td>1260</td>\n",
       "      <td>[1260]</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>texts/text_1760786502751.txt</td>\n",
       "      <td>1093</td>\n",
       "      <td>ascii</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>1093.0</td>\n",
       "      <td>1093</td>\n",
       "      <td>1093</td>\n",
       "      <td>[1093]</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>texts/text_1760786502826.txt</td>\n",
       "      <td>693</td>\n",
       "      <td>ascii</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>693.0</td>\n",
       "      <td>693</td>\n",
       "      <td>693</td>\n",
       "      <td>[693]</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>texts/text_1760786502909.txt</td>\n",
       "      <td>1046</td>\n",
       "      <td>ascii</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>1046.0</td>\n",
       "      <td>1046</td>\n",
       "      <td>1046</td>\n",
       "      <td>[1046]</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>texts/text_1760786502982.txt</td>\n",
       "      <td>1279</td>\n",
       "      <td>ascii</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>1279.0</td>\n",
       "      <td>1279</td>\n",
       "      <td>1279</td>\n",
       "      <td>[1279]</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>texts/text_1760786503082.txt</td>\n",
       "      <td>1041</td>\n",
       "      <td>utf-8</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>1036.0</td>\n",
       "      <td>1036</td>\n",
       "      <td>1036</td>\n",
       "      <td>[1036]</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>989 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              key  size_bytes encoding  empty  too_short  \\\n",
       "0    texts/text_1760786400687.txt        1043    ascii  False      False   \n",
       "1    texts/text_1760786400752.txt        1373    utf-8  False      False   \n",
       "2    texts/text_1760786400827.txt         734    utf-8  False      False   \n",
       "3    texts/text_1760786400902.txt        1072    ascii  False      False   \n",
       "4    texts/text_1760786400988.txt        1260    ascii  False      False   \n",
       "..                            ...         ...      ...    ...        ...   \n",
       "984  texts/text_1760786502751.txt        1093    ascii  False      False   \n",
       "985  texts/text_1760786502826.txt         693    ascii  False      False   \n",
       "986  texts/text_1760786502909.txt        1046    ascii  False      False   \n",
       "987  texts/text_1760786502982.txt        1279    ascii  False      False   \n",
       "988  texts/text_1760786503082.txt        1041    utf-8  False      False   \n",
       "\n",
       "     low_printable_ratio  lines  avg_tokens_per_paragraph  \\\n",
       "0                  False      1                    1043.0   \n",
       "1                  False      1                    1369.0   \n",
       "2                  False      1                     729.0   \n",
       "3                  False      1                    1072.0   \n",
       "4                  False      1                    1260.0   \n",
       "..                   ...    ...                       ...   \n",
       "984                False      1                    1093.0   \n",
       "985                False      1                     693.0   \n",
       "986                False      1                    1046.0   \n",
       "987                False      1                    1279.0   \n",
       "988                False      1                    1036.0   \n",
       "\n",
       "     max_tokens_paragraph  min_tokens_paragraph paragraph_token_list  \\\n",
       "0                    1043                  1043               [1043]   \n",
       "1                    1369                  1369               [1369]   \n",
       "2                     729                   729                [729]   \n",
       "3                    1072                  1072               [1072]   \n",
       "4                    1260                  1260               [1260]   \n",
       "..                    ...                   ...                  ...   \n",
       "984                  1093                  1093               [1093]   \n",
       "985                   693                   693                [693]   \n",
       "986                  1046                  1046               [1046]   \n",
       "987                  1279                  1279               [1279]   \n",
       "988                  1036                  1036               [1036]   \n",
       "\n",
       "    origin_language  \n",
       "0                en  \n",
       "1                en  \n",
       "2                en  \n",
       "3                en  \n",
       "4                en  \n",
       "..              ...  \n",
       "984              en  \n",
       "985              en  \n",
       "986              en  \n",
       "987              en  \n",
       "988              en  \n",
       "\n",
       "[989 rows x 12 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the data into a dataFrame\n",
    "df_data = pd.DataFrame(data)\n",
    "df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac0fd0c9-0257-408c-9a9e-e105f98f4807",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(bucket, key):\n",
    "    resp = s3.get_object(Bucket=bucket, Key=key)\n",
    "    body = resp[\"Body\"].read()\n",
    "    text = body.decode(\"utf-8\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cb57e739-1aba-4fa8-9891-d8b0a431e306",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Translation failed (Let me first state that the combat in this game has a certain degree of difficulty and is not suitable for friends who like to play idle games. Since there are not many testers, I am mainly testing myself. Even if I play it many times, there will inevitably be omissions and unreasonable things. If there are bugs or you think the difficulty is unreasonable in some places, I hope you can give positive feedback, leave a message in the comment area or join the group @! Thank you for playing! The game has a combat mechanism that is different from other turn-based games. Each character has unique talent points that can produce qualitative changes. You can train the character in the direction you want. But note that your talents need to be matched with the skills you mainly use to achieve maximum effect. Points to note in this game: 1. Many skills require TP points to release. We can improve the efficiency of TP acquisition by increasing attack speed or TP recovery. 2. Every time you use a skill, you will gain proficiency. Proficiency can increase skill level and damage. When skill proficiency meets the requirements, more advanced skills will be unlocked, so we need to use as many skills as possible. Practicing a skill to the extreme will be much stronger than practicing each skill for a while! 3. The control skills in the game are effective for any enemy, but some monsters will go berserk after being controlled. In the violent state, they will be immune to control and the damage may be slightly increased. 4. Press Q to save quickly. 5. The settings include functions such as setting combat acceleration and modifying window styles. 6. When changing equipment, the embedded items will be automatically transferred to the new equipment, and there is no need to remove them and re-embed them yourself. About combat: Unlike other turn-based combat, there is an attack speed attribute in the game. Attack speed directly affects the number of times a character's normal attacks cause damage, and each normal attack causes damage to restore 5 TP points. Physical skills have no cooling time and are only limited by TP points, so attack speed also indirectly affects the frequency of skill release. For characters on the agile route, if you want to increase output, increasing agility and attack speed is the best way. (The premise is that your attack cannot be too low) For characters with strength and intelligence routes, the increase in attack speed is not obvious, so TP recovery is crucial. In many cases, directly increasing damage is not as effective as increasing TP recovery!\n",
      "\n",
      "Regarding art: At first, I had the intention of hiring painters and artists, but after understanding the price, I gave up... The entire game includes character drawings, walking pictures, monster pictures, scene tiles, skill effects, UI design, and a bunch of icons that feel like there are hundreds of thousands of them, which makes me really unbearable. Later, I tried to find someone to cooperate with me and give me a share, but it seemed unrealistic. After all, it was very difficult to find someone to play your game for free... So, I could only bite the bullet and mess around on my own. Most of the UI materials of the game are from RPG Make and DLC, and some other resources are also used. After a lot of effort, it can only be made into what it is now! About the plot: As an RPG game, the plot is very important, but it is a pity... I personally think that the plot framework of the game is good, but limited by my own writing and lack of picture materials, many plots are actually compiled temporarily based on existing picture materials. In fact, there are many things that are not mentioned in the game, such as: 1. Regarding the origin of Sui Dou, in fact, Zoe and Sui Dou's parents were also demon hunters. Later, Sui Dou's parents died unexpectedly, so Zoe retired to take care of the protagonist. 2. Ye Tong's father was actually a general who was defeated by Xing Xuan more than ten years ago. The entire army died in the battle without a single survivor. Due to the inexplicable death of the group, he turned from a respected general to a laughing stock. Nighteye and his mother were also ridiculed by others, and Nighteye's mother also died of depression. So when he grew up, Nighteye came to Pallas to find out what happened back then and clear his father's name. 3. Why did Unos want to fight Pallas? In fact, it was someone who wanted to fight. This person, like Medivh, the mastermind behind the story, actually came from another continent. They just want to open fire between the two countries and let the two countries fight, so that they can take the opportunity to conduct experiments and create monsters for combat. To put it bluntly, they want to use this continent as a war laboratory. 4. In fact, there is a plot in the follow-up.\n",
      "\n",
      "The general content is: the two countries still fight, and then they meet Ye Tong again, but they have to fight each other. Then a new character will appear and stop the battle. The new character is a person from another continent. He is investigating an evil organization in their continent. This war is controlled by this evil organization behind the scenes in order to take advantage of the chaos and create controllable monsters... In fact, the whole plot is somewhat based on the Japanese anime \"Big Sword\", which is a banned masterpiece (I personally think it is a masterpiece!). I still have the resources of this anime on my computer. If you want to watch it, you can join the group @me, hahaha... Follow-up: Although this game has a follow-up plot, I don't plan to make a sequel for the time being, because I feel that I am not yet capable of telling the rest of the story well... Maybe one day when the time is right, I will come back and continue to fill this hole, maybe... I will continue to make new games in the future. I currently have some ideas, which is roughly a strategy management + war chess game. For the new game, I plan to use Unity to make it, and I will also spend money to hire an art painter. I will also notify everyone in the group of new progress, and I hope everyone can pay attention to it. Game QQ group: 634019932 Author b station homepage: Toutiao and Xigua video: --> Text length need to be between 0 and 5000 characters), keeping original text.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_CTRL_RE = re.compile(r\"[\\x00-\\x08\\x0B-\\x0C\\x0E-\\x1F\\x7F]\")          # control character\n",
    "_WS_RE = re.compile(r\"[ \\t\\u00A0\\u2000-\\u200B\\u3000]+\")\n",
    "HIGH_UNICODE_GARBAGE_RE = re.compile(r\"[^\\x00-\\x7F]+\")\n",
    "LANG_MAP = {\n",
    "    \"zh-cn\": \"zh-CN\",\n",
    "    \"zh_cn\": \"zh-CN\",\n",
    "    \"zh-CN\": \"zh-CN\",\n",
    "    \"zh\": \"zh-CN\",\n",
    "    \"en\": \"en\",\n",
    "    \"es\": \"es\",\n",
    "    \"fr\": \"fr\",\n",
    "    \"ja\": \"ja\",\n",
    "    \"ko\": \"ko\"\n",
    "}\n",
    "\n",
    "_SENT_SPLIT_RE = re.compile(\n",
    "    r'(?:\\s*\\n+\\s*)|'                 # paragraph/newline breaks\n",
    "    r'(?<=[。！？!?])\\s+'             # CJK sentence enders + whitespace\n",
    "    r'|(?<=[\\.\\?\\!])\\s+'              # English . ? ! + whitespace\n",
    ")\n",
    "\n",
    "\n",
    "try:\n",
    "    import emoji\n",
    "    EMOJI_RE = emoji.get_emoji_regexp()\n",
    "except Exception:\n",
    "    EMOJI_RE = re.compile(r'[\\U0001F300-\\U0001FAFF\\U00002700-\\U000027BF]+')\n",
    "\n",
    "\n",
    "COMBINING_MARKS_RE = re.compile(r'[\\u0300-\\u036F]+')     # combining marks\n",
    "\n",
    "def basic_clean(text: str) -> str:\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "    text = text.replace(\"\\ufeff\", \"\")\n",
    "    text = _CTRL_RE.sub(\"\", text)\n",
    "    text = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "    text = \"\\n\".join(_WS_RE.sub(\" \", ln).strip() for ln in text.split(\"\\n\"))\n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text) \n",
    "    return text\n",
    "\n",
    "    \n",
    "def to_english(text: str, lang) -> str:\n",
    "    try:\n",
    "        src_lang = LANG_MAP.get(lang.lower(), lang)\n",
    "        result =  GoogleTranslator(source=src_lang, target='english').translate(text)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Translation failed ({e}), keeping original text.\")\n",
    "        return text\n",
    "\n",
    "def split_into_sentences(text: str) -> List[str]:\n",
    "\n",
    "    text = re.sub(r'\\r\\n?', '\\n', text).strip()\n",
    "    parts = []\n",
    "    for para in filter(None, text.split('\\n')):\n",
    "        para = re.sub(r'\\s+', ' ', para).strip()\n",
    "        if not para:\n",
    "            continue\n",
    "        parts.extend([s.strip() for s in re.split(_SENT_SPLIT_RE, para) if s.strip()])\n",
    "    return parts\n",
    "\n",
    "def chunk_text_with_token_budget(\n",
    "    text: str,\n",
    "    tokenizer,\n",
    "    max_tokens: int = 512,\n",
    "    stride: int = 64,\n",
    "    reserve_special: int = 2,  # reserve tokens for [CLS]/[SEP] or similar\n",
    ") -> List[Dict]:\n",
    "    \n",
    "    # The actual available token budget after reserving special tokens\n",
    "    budget = max_tokens - reserve_special\n",
    "    assert budget > 0, \"max_tokens is too small; cannot reserve special tokens\"\n",
    "    \n",
    "    # Split text into sentences\n",
    "    sents = split_into_sentences(text)\n",
    "\n",
    "    chunks: List[Dict] = []\n",
    "    curr_sents: List[str] = []\n",
    "    curr_tokens = 0\n",
    "\n",
    "    # Helper: count token length for a given text (excluding special tokens)\n",
    "    def token_len(txt: str) -> int:\n",
    "        return len(tokenizer.encode(txt))\n",
    "\n",
    "    # Precompute token lengths of all sentences for efficiency\n",
    "    sent_lens = [token_len(s) for s in sents]\n",
    "\n",
    "    i = 0\n",
    "    while i < len(sents):\n",
    "        s = sents[i]\n",
    "        s_len = sent_lens[i]\n",
    "\n",
    "        # Case A: single sentence exceeds budget → split within sentence (sliding window)\n",
    "        if s_len > budget:\n",
    "            # If current chunk buffer is not empty, finalize it first\n",
    "            if curr_sents:\n",
    "                chunk_text = \" \".join(curr_sents).strip()\n",
    "                chunks.append({\"text\": chunk_text, \"n_tokens\": token_len(chunk_text), \"input_ids\": None})\n",
    "                curr_sents, curr_tokens = [], 0\n",
    "\n",
    "            # Encode the long sentence into token IDs\n",
    "            input_ids = tokenizer.encode(s)\n",
    "            start = 0\n",
    "            while start < len(input_ids):\n",
    "                # Take a slice of up to `budget` tokens\n",
    "                end = min(start + budget, len(input_ids))\n",
    "                piece_ids = input_ids[start:end]\n",
    "                # Decode back to text for saving\n",
    "                piece_text = tokenizer.decode(piece_ids).strip()\n",
    "                chunks.append({\"text\": piece_text, \"n_tokens\": len(piece_ids), \"input_ids\": None})\n",
    "\n",
    "                # If reached the end, stop\n",
    "                if end == len(input_ids):\n",
    "                    break\n",
    "                # Move window forward with overlap (`stride`)\n",
    "                start = max(end - stride, start + 1)\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        # Case B: greedy packing — keep adding sentences to current chunk\n",
    "        if curr_tokens + s_len <= budget:\n",
    "            curr_sents.append(s)\n",
    "            curr_tokens += s_len\n",
    "            i += 1\n",
    "        else:\n",
    "            # Finalize the current chunk when adding the next sentence would exceed the budget\n",
    "            chunk_text = \" \".join(curr_sents).strip()\n",
    "            chunks.append({\"text\": chunk_text, \"n_tokens\": token_len(chunk_text), \"input_ids\": None})\n",
    "            curr_sents, curr_tokens = [], 0\n",
    "\n",
    "    # Finalize any remaining sentences\n",
    "    if curr_sents:\n",
    "        chunk_text = \" \".join(curr_sents).strip()\n",
    "        chunks.append({\"text\": chunk_text, \"n_tokens\": token_len(chunk_text), \"input_ids\": None})\n",
    "\n",
    "    return chunks\n",
    "        \n",
    "def clean_text(df,max_tokens = 512):\n",
    "    bucket=\"trusted-zone\"\n",
    "    tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    for row in df.itertuples(index = False):\n",
    "\n",
    "        \n",
    "        #check text need remove or clean\n",
    "        if row.empty or row.too_short or row.low_printable_ratio:\n",
    "            client.delete_object(Bucket=bucket, Key=row.key)\n",
    "            continue\n",
    "        #get text\n",
    "        text = get_text(\"trusted-zone\",row.key)        \n",
    "        #basic clean and get text\n",
    "        text = basic_clean(text)\n",
    "\n",
    "        #if language of text isn't english tranlate it.\n",
    "        if(row.origin_language != \"en\"):\n",
    "            text = to_english(text, row.origin_language)\n",
    "\n",
    "        paragraphs = [p.strip() for p in text.split(\"\\n\\n\") if p.strip()]\n",
    "        for i in range (0, len(row.paragraph_token_list)):\n",
    "\n",
    "            if (row.paragraph_token_list[i] > max_tokens):\n",
    "                chunks = chunk_text_with_token_budget(paragraphs[i], tokenizer, max_tokens)\n",
    "                full_text = \"\\n\\n\".join(chunk[\"text\"].strip() for chunk in chunks if chunk[\"text\"].strip())\n",
    "                paragraphs[i]=full_text\n",
    "\n",
    "        text = \"\\n\\n\".join(paragraphs)\n",
    "        fixed = fix_text(text)\n",
    "\n",
    "        fixed = unicodedata.normalize(\"NFKC\", fixed)\n",
    "        fixed = COMBINING_MARKS_RE.sub(\"\", fixed)\n",
    "        fixed = _CTRL_RE.sub(\" \", fixed)\n",
    "\n",
    "        fixed = EMOJI_RE.sub(\" \", fixed)\n",
    "        fixed = HIGH_UNICODE_GARBAGE_RE.sub(\" \", fixed)\n",
    "        s3.put_object(\n",
    "            Bucket=bucket,\n",
    "            Key=row.key,  # Make sure the file key (path) is correct\n",
    "            Body=fixed.encode('utf-8'),\n",
    "            ContentType=\"text/plain\"\n",
    "        )\n",
    "\n",
    "        \n",
    "        \n",
    "            \n",
    "clean_text(df_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
