{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f8d07226-0c4f-46cf-8620-3d13019ee2b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Importing useful dependencies\n",
    "import re\n",
    "import unicodedata\n",
    "import io\n",
    "import os\n",
    "import boto3\n",
    "import chardet\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "from langdetect import detect\n",
    "from googletrans import Translator\n",
    "import asyncio\n",
    "from deep_translator import GoogleTranslator\n",
    "# set tokenizer with openAI standard token\n",
    "from typing import List, Dict\n",
    "from ftfy import fix_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "def1c220-1a48-4f4e-bf08-8dabcf4c488f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup S3 client for MinIO (MinIO implements Amazon S3 API)\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    endpoint_url=\"http://127.0.0.1:9000\", # MinIO API endpoint\n",
    "    aws_access_key_id=\"minioadmin\", # User name\n",
    "    aws_secret_access_key=\"minioadmin\", # Password\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "66f0316a-3375-40e7-9102-e86b69124317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Quality checks ----\n",
    "def check_text_quality(body: bytes, key: str):\n",
    "    \"\"\"Return a simple dict of basic quality stats for one file.\"\"\"\n",
    "    if not body:\n",
    "        return {\"key\": key, \"empty\": True}\n",
    "    \n",
    "    # detect encoding and decode safely\n",
    "    guess = chardet.detect(body)\n",
    "    enc = guess.get(\"encoding\") or \"utf-8\"\n",
    "    text = body.decode(enc, errors=\"replace\")\n",
    "\n",
    "    # check printable ratio (avoid binary garbage)\n",
    "    printable_ratio = sum(c.isprintable() or c.isspace() for c in text) / max(1, len(text))\n",
    "    origin_lang = detect(text)\n",
    "    # --- Paragraph token stats ---\n",
    "    paragraphs = [p.strip() for p in text.split(\"\\n\\n\") if p.strip()]\n",
    "\n",
    "    paragraph_tokens = [len(p) for p in paragraphs] if paragraphs else []\n",
    "    \n",
    "    avg_tokens_per_paragraph = (\n",
    "        sum(paragraph_tokens) / len(paragraph_tokens) if paragraph_tokens else 0\n",
    "    )\n",
    "    max_tokens_paragraph = max(paragraph_tokens) if paragraph_tokens else 0\n",
    "    min_tokens_paragraph = min(paragraph_tokens) if paragraph_tokens else 0\n",
    "    # basic stats\n",
    "    return {\n",
    "        \"key\": key,\n",
    "        \"size_bytes\": len(body),\n",
    "        \"encoding\": enc,\n",
    "        \"empty\": not bool(text.strip()),\n",
    "        \"too_short\": len(text.strip()) < 20,\n",
    "        \"low_printable_ratio\": printable_ratio < 0.9,\n",
    "        \"lines\": len(text.splitlines()),\n",
    "        \"avg_tokens_per_paragraph\": round(avg_tokens_per_paragraph, 2),\n",
    "        \"max_tokens_paragraph\": max_tokens_paragraph,\n",
    "        \"min_tokens_paragraph\": min_tokens_paragraph,\n",
    "        \"paragraph_token_list\": paragraph_tokens,  # you can drop this if you don’t need full list\n",
    "        \"origin_language\": origin_lang,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6f1e5910-5edf-4b0d-9706-911d91a9a043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Run checks on all txt files ----\n",
    "def extract_datas(bucket,prefix=\"\"):\n",
    "    results = []\n",
    "    paginator = s3.get_paginator(\"list_objects_v2\")\n",
    "    for page in paginator.paginate(Bucket=bucket, Prefix=prefix):\n",
    "        for obj in page.get(\"Contents\", []):\n",
    "            \n",
    "            key = obj[\"Key\"]\n",
    "\n",
    "            if obj['Size'] == 0 and key.endswith(\"/\"): # skip the folder itself\n",
    "                continue\n",
    "            # Download the text\n",
    "            resp = s3.get_object(Bucket=bucket, Key=key)\n",
    "            body = resp[\"Body\"].read()\n",
    "            stats = check_text_quality(body, key)\n",
    "            results.append(stats)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f2054ca-2f79-4ba8-9650-bd7fc09a0a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = extract_datas(bucket = \"trusted-zone\", prefix = \"texts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "86b9d6a3-cd3a-439d-82be-e140759a4504",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>size_bytes</th>\n",
       "      <th>encoding</th>\n",
       "      <th>empty</th>\n",
       "      <th>too_short</th>\n",
       "      <th>low_printable_ratio</th>\n",
       "      <th>lines</th>\n",
       "      <th>avg_tokens_per_paragraph</th>\n",
       "      <th>max_tokens_paragraph</th>\n",
       "      <th>min_tokens_paragraph</th>\n",
       "      <th>paragraph_token_list</th>\n",
       "      <th>origin_language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>texts/text_1760137269318.txt</td>\n",
       "      <td>1043</td>\n",
       "      <td>ascii</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>1043.0</td>\n",
       "      <td>1043</td>\n",
       "      <td>1043</td>\n",
       "      <td>[1043]</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>texts/text_1760137269387.txt</td>\n",
       "      <td>1373</td>\n",
       "      <td>utf-8</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>1369.0</td>\n",
       "      <td>1369</td>\n",
       "      <td>1369</td>\n",
       "      <td>[1369]</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>texts/text_1760137269455.txt</td>\n",
       "      <td>734</td>\n",
       "      <td>utf-8</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>729.0</td>\n",
       "      <td>729</td>\n",
       "      <td>729</td>\n",
       "      <td>[729]</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>texts/text_1760137269523.txt</td>\n",
       "      <td>1260</td>\n",
       "      <td>ascii</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>1260.0</td>\n",
       "      <td>1260</td>\n",
       "      <td>1260</td>\n",
       "      <td>[1260]</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>texts/text_1760137269594.txt</td>\n",
       "      <td>1046</td>\n",
       "      <td>utf-8</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>1044.0</td>\n",
       "      <td>1044</td>\n",
       "      <td>1044</td>\n",
       "      <td>[1044]</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>589</th>\n",
       "      <td>texts/text_1760137314543.txt</td>\n",
       "      <td>580</td>\n",
       "      <td>ascii</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>580.0</td>\n",
       "      <td>580</td>\n",
       "      <td>580</td>\n",
       "      <td>[580]</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>590</th>\n",
       "      <td>texts/text_1760137314614.txt</td>\n",
       "      <td>944</td>\n",
       "      <td>ascii</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>944.0</td>\n",
       "      <td>944</td>\n",
       "      <td>944</td>\n",
       "      <td>[944]</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>591</th>\n",
       "      <td>texts/text_1760137314681.txt</td>\n",
       "      <td>2519</td>\n",
       "      <td>utf-8</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>2513.0</td>\n",
       "      <td>2513</td>\n",
       "      <td>2513</td>\n",
       "      <td>[2513]</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592</th>\n",
       "      <td>texts/text_1760137314745.txt</td>\n",
       "      <td>950</td>\n",
       "      <td>utf-8</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>944.0</td>\n",
       "      <td>944</td>\n",
       "      <td>944</td>\n",
       "      <td>[944]</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>593</th>\n",
       "      <td>texts/text_1760137314807.txt</td>\n",
       "      <td>4804</td>\n",
       "      <td>utf-8</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>4715.0</td>\n",
       "      <td>4715</td>\n",
       "      <td>4715</td>\n",
       "      <td>[4715]</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>594 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              key  size_bytes encoding  empty  too_short  \\\n",
       "0    texts/text_1760137269318.txt        1043    ascii  False      False   \n",
       "1    texts/text_1760137269387.txt        1373    utf-8  False      False   \n",
       "2    texts/text_1760137269455.txt         734    utf-8  False      False   \n",
       "3    texts/text_1760137269523.txt        1260    ascii  False      False   \n",
       "4    texts/text_1760137269594.txt        1046    utf-8  False      False   \n",
       "..                            ...         ...      ...    ...        ...   \n",
       "589  texts/text_1760137314543.txt         580    ascii  False      False   \n",
       "590  texts/text_1760137314614.txt         944    ascii  False      False   \n",
       "591  texts/text_1760137314681.txt        2519    utf-8  False      False   \n",
       "592  texts/text_1760137314745.txt         950    utf-8  False      False   \n",
       "593  texts/text_1760137314807.txt        4804    utf-8  False      False   \n",
       "\n",
       "     low_printable_ratio  lines  avg_tokens_per_paragraph  \\\n",
       "0                  False      1                    1043.0   \n",
       "1                  False      1                    1369.0   \n",
       "2                  False      1                     729.0   \n",
       "3                  False      1                    1260.0   \n",
       "4                  False      1                    1044.0   \n",
       "..                   ...    ...                       ...   \n",
       "589                False      1                     580.0   \n",
       "590                False      1                     944.0   \n",
       "591                False      1                    2513.0   \n",
       "592                False      1                     944.0   \n",
       "593                False      1                    4715.0   \n",
       "\n",
       "     max_tokens_paragraph  min_tokens_paragraph paragraph_token_list  \\\n",
       "0                    1043                  1043               [1043]   \n",
       "1                    1369                  1369               [1369]   \n",
       "2                     729                   729                [729]   \n",
       "3                    1260                  1260               [1260]   \n",
       "4                    1044                  1044               [1044]   \n",
       "..                    ...                   ...                  ...   \n",
       "589                   580                   580                [580]   \n",
       "590                   944                   944                [944]   \n",
       "591                  2513                  2513               [2513]   \n",
       "592                   944                   944                [944]   \n",
       "593                  4715                  4715               [4715]   \n",
       "\n",
       "    origin_language  \n",
       "0                en  \n",
       "1                en  \n",
       "2                en  \n",
       "3                en  \n",
       "4                en  \n",
       "..              ...  \n",
       "589              en  \n",
       "590              en  \n",
       "591              en  \n",
       "592              en  \n",
       "593              en  \n",
       "\n",
       "[594 rows x 12 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the data into a dataFrame\n",
    "df_data = pd.DataFrame(data)\n",
    "df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac0fd0c9-0257-408c-9a9e-e105f98f4807",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(bucket, key):\n",
    "    resp = s3.get_object(Bucket=bucket, Key=key)\n",
    "    body = resp[\"Body\"].read()\n",
    "    text = body.decode(\"utf-8\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cb57e739-1aba-4fa8-9891-d8b0a431e306",
   "metadata": {},
   "outputs": [],
   "source": [
    "_CTRL_RE = re.compile(r\"[\\x00-\\x08\\x0B-\\x0C\\x0E-\\x1F\\x7F]\")          # control character\n",
    "_WS_RE = re.compile(r\"[ \\t\\u00A0\\u2000-\\u200B\\u3000]+\")\n",
    "HIGH_UNICODE_GARBAGE_RE = re.compile(r\"[^\\x00-\\x7F]+\")\n",
    "LANG_MAP = {\n",
    "    \"zh-cn\": \"zh-CN\",\n",
    "    \"zh_cn\": \"zh-CN\",\n",
    "    \"zh-CN\": \"zh-CN\",\n",
    "    \"zh\": \"zh-CN\",\n",
    "    \"en\": \"en\",\n",
    "    \"es\": \"es\",\n",
    "    \"fr\": \"fr\",\n",
    "    \"ja\": \"ja\",\n",
    "    \"ko\": \"ko\"\n",
    "}\n",
    "\n",
    "_SENT_SPLIT_RE = re.compile(\n",
    "    r'(?:\\s*\\n+\\s*)|'                 # paragraph/newline breaks\n",
    "    r'(?<=[。！？!?])\\s+'             # CJK sentence enders + whitespace\n",
    "    r'|(?<=[\\.\\?\\!])\\s+'              # English . ? ! + whitespace\n",
    ")\n",
    "\n",
    "\n",
    "try:\n",
    "    import emoji\n",
    "    EMOJI_RE = emoji.get_emoji_regexp()\n",
    "except Exception:\n",
    "    EMOJI_RE = re.compile(r'[\\U0001F300-\\U0001FAFF\\U00002700-\\U000027BF]+')\n",
    "\n",
    "\n",
    "COMBINING_MARKS_RE = re.compile(r'[\\u0300-\\u036F]+')     # combining marks\n",
    "\n",
    "def basic_clean(text: str) -> str:\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "    text = text.replace(\"\\ufeff\", \"\")\n",
    "    text = _CTRL_RE.sub(\"\", text)\n",
    "    text = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "    text = \"\\n\".join(_WS_RE.sub(\" \", ln).strip() for ln in text.split(\"\\n\"))\n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text) \n",
    "    return text\n",
    "\n",
    "    \n",
    "def to_english(text: str, lang) -> str:\n",
    "    try:\n",
    "        src_lang = LANG_MAP.get(lang.lower(), lang)\n",
    "        result =  GoogleTranslator(source=src_lang, target='english').translate(text)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Translation failed ({e}), keeping original text.\")\n",
    "        return text\n",
    "\n",
    "def split_into_sentences(text: str) -> List[str]:\n",
    "\n",
    "    text = re.sub(r'\\r\\n?', '\\n', text).strip()\n",
    "    parts = []\n",
    "    for para in filter(None, text.split('\\n')):\n",
    "        para = re.sub(r'\\s+', ' ', para).strip()\n",
    "        if not para:\n",
    "            continue\n",
    "        parts.extend([s.strip() for s in re.split(_SENT_SPLIT_RE, para) if s.strip()])\n",
    "    return parts\n",
    "\n",
    "def chunk_text_with_token_budget(\n",
    "    text: str,\n",
    "    tokenizer,\n",
    "    max_tokens: int = 512,\n",
    "    stride: int = 64,\n",
    "    reserve_special: int = 2,  # reserve tokens for [CLS]/[SEP] or similar\n",
    ") -> List[Dict]:\n",
    "    \n",
    "    # The actual available token budget after reserving special tokens\n",
    "    budget = max_tokens - reserve_special\n",
    "    assert budget > 0, \"max_tokens is too small; cannot reserve special tokens\"\n",
    "    \n",
    "    # Split text into sentences\n",
    "    sents = split_into_sentences(text)\n",
    "\n",
    "    chunks: List[Dict] = []\n",
    "    curr_sents: List[str] = []\n",
    "    curr_tokens = 0\n",
    "\n",
    "    # Helper: count token length for a given text (excluding special tokens)\n",
    "    def token_len(txt: str) -> int:\n",
    "        return len(tokenizer.encode(txt))\n",
    "\n",
    "    # Precompute token lengths of all sentences for efficiency\n",
    "    sent_lens = [token_len(s) for s in sents]\n",
    "\n",
    "    i = 0\n",
    "    while i < len(sents):\n",
    "        s = sents[i]\n",
    "        s_len = sent_lens[i]\n",
    "\n",
    "        # Case A: single sentence exceeds budget → split within sentence (sliding window)\n",
    "        if s_len > budget:\n",
    "            # If current chunk buffer is not empty, finalize it first\n",
    "            if curr_sents:\n",
    "                chunk_text = \" \".join(curr_sents).strip()\n",
    "                chunks.append({\"text\": chunk_text, \"n_tokens\": token_len(chunk_text), \"input_ids\": None})\n",
    "                curr_sents, curr_tokens = [], 0\n",
    "\n",
    "            # Encode the long sentence into token IDs\n",
    "            input_ids = tokenizer.encode(s, add_special_tokens=False)\n",
    "            start = 0\n",
    "            while start < len(input_ids):\n",
    "                # Take a slice of up to `budget` tokens\n",
    "                end = min(start + budget, len(input_ids))\n",
    "                piece_ids = input_ids[start:end]\n",
    "                # Decode back to text for saving\n",
    "                piece_text = tokenizer.decode(piece_ids, skip_special_tokens=True).strip()\n",
    "                chunks.append({\"text\": piece_text, \"n_tokens\": len(piece_ids), \"input_ids\": None})\n",
    "\n",
    "                # If reached the end, stop\n",
    "                if end == len(input_ids):\n",
    "                    break\n",
    "                # Move window forward with overlap (`stride`)\n",
    "                start = max(end - stride, start + 1)\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        # Case B: greedy packing — keep adding sentences to current chunk\n",
    "        if curr_tokens + s_len <= budget:\n",
    "            curr_sents.append(s)\n",
    "            curr_tokens += s_len\n",
    "            i += 1\n",
    "        else:\n",
    "            # Finalize the current chunk when adding the next sentence would exceed the budget\n",
    "            chunk_text = \" \".join(curr_sents).strip()\n",
    "            chunks.append({\"text\": chunk_text, \"n_tokens\": token_len(chunk_text), \"input_ids\": None})\n",
    "            curr_sents, curr_tokens = [], 0\n",
    "\n",
    "    # Finalize any remaining sentences\n",
    "    if curr_sents:\n",
    "        chunk_text = \" \".join(curr_sents).strip()\n",
    "        chunks.append({\"text\": chunk_text, \"n_tokens\": token_len(chunk_text), \"input_ids\": None})\n",
    "\n",
    "    return chunks\n",
    "        \n",
    "def clean_text(df,max_tokens = 512):\n",
    "    bucket=\"trusted-zone\"\n",
    "    tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    for row in df.itertuples(index = False):\n",
    "\n",
    "        \n",
    "        #check text need remove or clean\n",
    "        if row.empty or row.too_short or row.low_printable_ratio:\n",
    "            client.delete_object(Bucket=bucket, Key=row.key)\n",
    "            continue\n",
    "        #get text\n",
    "        text = get_text(\"trusted-zone\",row.key)        \n",
    "        #basic clean and get text\n",
    "        text = basic_clean(text)\n",
    "\n",
    "        #if language of text isn't english tranlate it.\n",
    "        if(row.origin_language != \"en\"):\n",
    "            text = to_english(text, row.origin_language)\n",
    "\n",
    "        paragraphs = [p.strip() for p in text.split(\"\\n\\n\") if p.strip()]\n",
    "        for i in range (0, len(row.paragraph_token_list)):\n",
    "\n",
    "            if (row.paragraph_token_list[i] > max_tokens):\n",
    "                chunks = chunk_text_with_token_budget(paragraphs[i], tokenizer, max_tokens)\n",
    "                full_text = \"\\n\\n\".join(chunk[\"text\"].strip() for chunk in chunks if chunk[\"text\"].strip())\n",
    "                paragraphs[i]=full_text\n",
    "\n",
    "        text = \"\\n\\n\".join(paragraphs)\n",
    "        fixed = fix_text(text)\n",
    "\n",
    "        fixed = unicodedata.normalize(\"NFKC\", fixed)\n",
    "        fixed = COMBINING_MARKS_RE.sub(\"\", fixed)\n",
    "        fixed = _CTRL_RE.sub(\" \", fixed)\n",
    "\n",
    "        fixed = EMOJI_RE.sub(\" \", fixed)\n",
    "        fixed = HIGH_UNICODE_GARBAGE_RE.sub(\" \", fixed)\n",
    "        s3.put_object(\n",
    "            Bucket=bucket,\n",
    "            Key=row.key,  # Make sure the file key (path) is correct\n",
    "            Body=fixed.encode('utf-8'),\n",
    "            ContentType=\"text/plain\"\n",
    "        )\n",
    "\n",
    "        \n",
    "        \n",
    "            \n",
    "clean_text(df_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
