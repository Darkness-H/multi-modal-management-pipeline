{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b3cbfd3-0493-40c8-9b16-ffaeb9aca44b",
   "metadata": {},
   "source": [
    "## Task 2: Multi-Modality Similarity Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440a5a3d-a043-40b6-be5b-1ce15bbfa90f",
   "metadata": {},
   "source": [
    "The **second task** we define for our **Game Recommendation Assistant** is the following:\n",
    "\n",
    "**Text-based retrieval**: Given a game description or textual query such as “open-world fantasy adventure”, the system retrieves game covers (images), trailers (videos), and descriptions (texts) of games with similar themes, genres, or narrative elements.\n",
    "\n",
    "**Image-based retrieval**: Given a game cover or in-game snapshot, the system retrieves visually and semantically related games, including similar covers (images), trailers (videos), and descriptions (texts) that share comparable art styles, visual motifs, or atmosphere.\n",
    "\n",
    "**Video-based retrieval**: Given a game trailer, the system retrieves trailers (videos), covers (images), and descriptions (texts) of games with a similar visual tone, gameplay style, or mood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d4cef9e-7253-482b-8264-d2741ff719f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing useful dependencies\n",
    "import io\n",
    "import boto3\n",
    "import torch\n",
    "import chromadb\n",
    "import open_clip\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bb690be-ee17-4164-93a2-812913afcc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup S3 client for MinIO (MinIO implements Amazon S3 API)\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    endpoint_url=\"http://127.0.0.1:9000\", # MinIO API endpoint\n",
    "    aws_access_key_id=\"minioadmin\", # User name\n",
    "    aws_secret_access_key=\"minioadmin\", # Password\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "940d799b-3630-497d-8a25-a537ef112260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the server of ChromaDB where we stored the embeddings of files (Docker Container)\n",
    "client = chromadb.HttpClient(host=\"localhost\", port=8000)\n",
    "\n",
    "# Create or get the collection named \"texts_images\"\n",
    "collection_texts_images = client.create_collection(name=\"texts_images\", get_or_create=True, embedding_function=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c13e583-172f-462a-b0b7-7b8cc3421ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just in case our device has gpu\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66f63b2-a2f3-4386-89df-165bb9564c00",
   "metadata": {},
   "source": [
    "Let's define the models we will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89c3f493-e0d7-4ab9-92d7-5305ef14b11a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\open_clip\\factory.py:450: UserWarning: QuickGELU mismatch between final model config (quick_gelu=False) and pretrained tag 'openai' (quick_gelu=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CLIP(\n",
       "  (visual): VisionTransformer(\n",
       "    (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)\n",
       "    (patch_dropout): Identity()\n",
       "    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): ModuleList(\n",
       "        (0-11): 12 x ResidualAttentionBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ls_1): Identity()\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): GELU(approximate='none')\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ls_2): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (resblocks): ModuleList(\n",
       "      (0-11): 12 x ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ls_1): Identity()\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ls_2): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (token_embedding): Embedding(49408, 512)\n",
       "  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load \"ViT-B-16\" model for images and texts\n",
    "model_it, _, preprocess_it = open_clip.create_model_and_transforms(\"ViT-B-16\", pretrained=\"openai\")\n",
    "tokenizer_it = open_clip.get_tokenizer(\"ViT-B-16\") # Tokenizer for texts\n",
    "model_it.to(device)\n",
    "\n",
    "# +++"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741f3a44-4780-4a66-9caa-0888092c0e29",
   "metadata": {},
   "source": [
    "In the following cells we implement some functions to get embeddings of a given data and get files from MinIO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66953329-2934-4f00-82b5-bd2982107fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use this function to retrieve a text from our bucket\n",
    "def get_text(bucket, key):\n",
    "    response = s3.get_object(Bucket=bucket, Key=key)\n",
    "    body = response[\"Body\"].read().decode(\"utf-8\")\n",
    "    return body\n",
    "\n",
    "# We can use this function to retrieve an image from our bucket in PIL Image format\n",
    "def get_image(bucket, key):\n",
    "    resp = s3.get_object(Bucket=bucket, Key=key)\n",
    "    body = resp[\"Body\"].read()\n",
    "    img = Image.open(io.BytesIO(body))\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28a06f67-0eeb-4250-9137-922d370b192f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "# The next function returns the embedding of the given text\n",
    "def embed_text(preprocess, model, tokenizer, texts: str):\n",
    "    tokens = tokenizer([texts]).to(device) # tokenized batch\n",
    "    feats = model.encode_text(tokens)\n",
    "    feats = feats / feats.norm(dim=-1, keepdim=True) # normalize\n",
    "    return feats.cpu().numpy()[0]\n",
    "\n",
    "# The next function returns the embedding of the given PIL Image\n",
    "def embed_image(preprocess, model, pil_img):\n",
    "    img_tensor = preprocess(pil_img).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        feats = model.encode_image(img_tensor)\n",
    "    feats = feats / feats.norm(dim=-1, keepdim=True)\n",
    "    return feats.cpu().numpy().squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb1a275-6331-43ab-a753-d4f20d36d85c",
   "metadata": {},
   "source": [
    "Here below, we implement some functions to retrieve similar multi-modality data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b489da4-2289-40eb-9cac-ad257d0bf4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_k_images_texts(res, k=5):\n",
    "    # Print results with type (text/image)\n",
    "    n_text = 0\n",
    "    n_image = 0\n",
    "    i = 0\n",
    "    for _, doc in enumerate(res[\"documents\"][0]):\n",
    "        if (doc.split(\".\")[-1] == \"txt\" and n_text < k):\n",
    "            print(f\"{i+1}. Distance: {res['distances'][0][i]:.4f}\")\n",
    "            print(\"Content:\", doc)\n",
    "            print(get_text(\"trusted-zone\", doc.replace(\"trusted-zone/\", \"\", 1)))\n",
    "            print(\"-\" * 40)\n",
    "            i += 1\n",
    "            n_text += 1\n",
    "        elif (doc.split(\".\")[-1] == \"png\" and n_image < k):\n",
    "            print(f\"{i+1}. Distance: {res['distances'][0][i]:.4f}\")\n",
    "            print(\"Content:\", doc)\n",
    "            display(get_image(\"trusted-zone\", doc.replace(\"trusted-zone/\", \"\", 1)))\n",
    "            print(\"-\" * 40)\n",
    "            i += 1\n",
    "            n_image += 1\n",
    "        # Stop early if both top-k limits are reached\n",
    "        if n_text >= k and n_image >= k:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed24da8-d43b-4f42-ab6d-fdc9805cacef",
   "metadata": {},
   "source": [
    "**Text-based retrieval**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6641d59d-5e1c-4f29-a7a9-936dd68a6099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: query by another game's description\n",
    "query_text = \"Games similar to Nier: Automata\"\n",
    "q_vec = embed_text(preprocess_it, model_it, tokenizer_it, query_text).tolist()\n",
    "\n",
    "res = collection_texts_images.query(\n",
    "    query_embeddings=[q_vec],\n",
    "    # It’s expected that all nearest neighbors are text for a long text query.\n",
    "    # To get images, we need to retrieve more embeddings.\n",
    "    n_results=1000,\n",
    "    include=[\"documents\",\"distances\"]\n",
    ")\n",
    "\n",
    "print_top_k_images_texts(res, k = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9e570f-a7a1-410e-84b6-3f2e18bcac13",
   "metadata": {},
   "source": [
    "**Image-based retrieval**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ed5da67-de99-47fa-84b9-4d0cdcbb93ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f333ad6939b844dd88baecd69c1781a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value=(), accept='image/*', description='Upload')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Upload an image from local storage\n",
    "uploader = widgets.FileUpload(accept='image/*', multiple=False)\n",
    "display(uploader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac069364-4014-48e6-826c-6d0de8dab588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the uploaded file\n",
    "image_data = uploader.value[0].content\n",
    "img_example = Image.open(BytesIO(image_data))\n",
    "\n",
    "# Create embeddings for the Image\n",
    "img_example_emb = embed_image(preprocess_it, model_it, img_example)\n",
    "img_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "02a1e0cb-074d-43c8-9127-586c633850c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: query by another game's cover or snapshot\n",
    "q_vec = embed_image(preprocess_it, model_it, img_example)\n",
    "\n",
    "res = collection_texts_images.query(\n",
    "    query_embeddings=[q_vec],\n",
    "    n_results=1000,\n",
    "    include=[\"documents\",\"distances\"]\n",
    ")\n",
    "\n",
    "print_top_k_images_texts(res, k = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb99f881-495c-452a-aff3-59a3374dd66c",
   "metadata": {},
   "source": [
    "**Video-based retrieval**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
