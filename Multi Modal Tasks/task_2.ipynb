{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b3cbfd3-0493-40c8-9b16-ffaeb9aca44b",
   "metadata": {},
   "source": [
    "## Task 2: Multi-Modality Similarity Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440a5a3d-a043-40b6-be5b-1ce15bbfa90f",
   "metadata": {},
   "source": [
    "The **second task** we defined for our **Game Recommendation Assistant** is the following:\n",
    "\n",
    "**Text-based retrieval**: Given a game description or textual query such as \"open-world fantasy adventure\", the system retrieves game covers (images), trailers (videos), and descriptions (texts) of games with similar themes, genres, or narrative elements.\n",
    "\n",
    "**Image-based retrieval**: Given a game cover or in-game snapshot, the system retrieves visually and semantically related games, including similar covers (images), trailers (videos), and descriptions (texts) that share comparable art styles, visual motifs, or atmosphere.\n",
    "\n",
    "**Video-based retrieval**: Given a game trailer, the system retrieves trailers (videos), covers (images), and descriptions (texts) of games with a similar visual tone, gameplay style, or mood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4cef9e-7253-482b-8264-d2741ff719f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing useful dependencies\n",
    "import io\n",
    "import boto3\n",
    "import torch\n",
    "import requests\n",
    "import imageio\n",
    "import chromadb\n",
    "import open_clip\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "from io import BytesIO\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from transformers import CLIPVisionModelWithProjection, CLIPImageProcessor, AutoTokenizer, CLIPTextModelWithProjection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb690be-ee17-4164-93a2-812913afcc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup S3 client for MinIO (MinIO implements Amazon S3 API)\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    endpoint_url=\"http://127.0.0.1:9000\", # MinIO API endpoint\n",
    "    aws_access_key_id=\"minioadmin\", # User name\n",
    "    aws_secret_access_key=\"minioadmin\", # Password\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940d799b-3630-497d-8a25-a537ef112260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the server of ChromaDB where we stored the embeddings of files (Docker Container)\n",
    "client = chromadb.HttpClient(host=\"localhost\", port=8000)\n",
    "\n",
    "# Create or get the collection named \"texts_images\"\n",
    "collection_texts_images = client.create_collection(name=\"texts_images\", get_or_create=True, embedding_function=None)\n",
    "\n",
    "# Create or get the collection named \"texts_images_videos\"\n",
    "collection_texts_images_videos = client.create_collection(name=\"texts_images_videos\", get_or_create=True, embedding_function=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c13e583-172f-462a-b0b7-7b8cc3421ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just in case our device has gpu\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66f63b2-a2f3-4386-89df-165bb9564c00",
   "metadata": {},
   "source": [
    "Let's define the models we will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c3f493-e0d7-4ab9-92d7-5305ef14b11a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load \"ViT-B-16\" model for images and texts\n",
    "model_it, _, preprocess_it = open_clip.create_model_and_transforms(\"ViT-B-16\", pretrained=\"openai\")\n",
    "tokenizer_it = open_clip.get_tokenizer(\"ViT-B-16\") # Tokenizer for texts\n",
    "model_it.to(device)\n",
    "\n",
    "# =================================================================================================\n",
    "\n",
    "# Load \"Searchium-ai/clip4clip-webvid150k\" for texts, images and videos\n",
    "\n",
    "# === Text encoder ===\n",
    "text_tokenizer = AutoTokenizer.from_pretrained(\"Searchium-ai/clip4clip-webvid150k\")\n",
    "text_model = CLIPTextModelWithProjection.from_pretrained(\"Searchium-ai/clip4clip-webvid150k\")\n",
    "text_model.to(device)\n",
    "\n",
    "# === Image / video frame encoder ===\n",
    "image_processor = CLIPImageProcessor.from_pretrained(\"Searchium-ai/clip4clip-webvid150k\")\n",
    "vision_model = CLIPVisionModelWithProjection.from_pretrained(\"Searchium-ai/clip4clip-webvid150k\")\n",
    "vision_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637f9ab5-15d6-4910-b5de-cee3da67e27d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Using **ViT-B-16**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741f3a44-4780-4a66-9caa-0888092c0e29",
   "metadata": {},
   "source": [
    "In the following cells we implement some functions to get embeddings of a given data and get files from MinIO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66953329-2934-4f00-82b5-bd2982107fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use this function to retrieve a text from our bucket\n",
    "def get_text(bucket, key):\n",
    "    response = s3.get_object(Bucket=bucket, Key=key)\n",
    "    body = response[\"Body\"].read().decode(\"utf-8\")\n",
    "    return body\n",
    "\n",
    "# We can use this function to retrieve an image from our bucket in PIL Image format\n",
    "def get_image(bucket, key):\n",
    "    resp = s3.get_object(Bucket=bucket, Key=key)\n",
    "    body = resp[\"Body\"].read()\n",
    "    img = Image.open(io.BytesIO(body))\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a06f67-0eeb-4250-9137-922d370b192f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "# The next function returns the embedding of the given text\n",
    "def embed_text(model, tokenizer, texts: str):\n",
    "    tokens = tokenizer([texts]).to(device) # tokenized batch\n",
    "    feats = model.encode_text(tokens)\n",
    "    feats = feats / feats.norm(dim=-1, keepdim=True) # normalize\n",
    "    return feats.cpu().numpy()[0]\n",
    "\n",
    "# The next function returns the embedding of the given PIL Image\n",
    "def embed_image(preprocess, model, pil_img):\n",
    "    img_tensor = preprocess(pil_img).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        feats = model.encode_image(img_tensor)\n",
    "    feats = feats / feats.norm(dim=-1, keepdim=True)\n",
    "    return feats.cpu().numpy().squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb1a275-6331-43ab-a753-d4f20d36d85c",
   "metadata": {},
   "source": [
    "Here below, we implement some functions to retrieve similar multi-modality data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b489da4-2289-40eb-9cac-ad257d0bf4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_k_files(res, k=5, videos=False):\n",
    "    # Print results with type (text/image/video)\n",
    "    n_text = 0\n",
    "    n_image = 0\n",
    "    n_video = 0\n",
    "    i = 0\n",
    "    for _, doc in enumerate(res[\"documents\"][0]):\n",
    "        if (doc.split(\".\")[-1] == \"txt\" and n_text < k):\n",
    "            print(f\"{i+1}. Distance: {res['distances'][0][i]:.4f}\")\n",
    "            print(\"Content:\", doc)\n",
    "            print(get_text(\"trusted-zone\", doc.replace(\"trusted-zone/\", \"\", 1)))\n",
    "            print(\"-\" * 40)\n",
    "            i += 1\n",
    "            n_text += 1\n",
    "        elif (doc.split(\".\")[-1] == \"png\" and n_image < k):\n",
    "            print(f\"{i+1}. Distance: {res['distances'][0][i]:.4f}\")\n",
    "            print(\"Content:\", doc)\n",
    "            display(get_image(\"trusted-zone\", doc.replace(\"trusted-zone/\", \"\", 1)))\n",
    "            print(\"-\" * 40)\n",
    "            i += 1\n",
    "            n_image += 1\n",
    "\n",
    "        if videos:\n",
    "            if (doc.split(\".\")[-1] == \"mp4\" and n_video < k):\n",
    "                print(f\"{i+1}. Distance: {res['distances'][0][i]:.4f}\")\n",
    "                print(\"Content:\", doc)\n",
    "                frames = get_video(\"trusted-zone\", doc.replace(\"trusted-zone/\", \"\", 1))\n",
    "                for frame in frames:\n",
    "                    display(frame)\n",
    "                print(\"-\" * 40)\n",
    "                i += 1\n",
    "                n_video += 1\n",
    "                \n",
    "            # Stop early if both top-k limits are reached\n",
    "            if n_text >= k and n_image >= k and n_video >= k:\n",
    "                break\n",
    "        else:\n",
    "            # Stop early if both top-k limits are reached\n",
    "            if n_text >= k and n_image >= k:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed24da8-d43b-4f42-ab6d-fdc9805cacef",
   "metadata": {},
   "source": [
    "**Text-based retrieval**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6641d59d-5e1c-4f29-a7a9-936dd68a6099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: query by another game's description\n",
    "query_text = \"Games similar to Nier: Automata\"\n",
    "q_vec = embed_text(model_it, tokenizer_it, query_text).tolist()\n",
    "\n",
    "res = collection_texts_images.query(\n",
    "    query_embeddings=[q_vec],\n",
    "    # It’s expected that all nearest neighbors are text for a long text query.\n",
    "    # To get images, we need to retrieve more embeddings.\n",
    "    n_results=2000,\n",
    "    include=[\"documents\",\"distances\"]\n",
    ")\n",
    "\n",
    "print_top_k_files(res, k = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9e570f-a7a1-410e-84b6-3f2e18bcac13",
   "metadata": {},
   "source": [
    "**Image-based retrieval**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed5da67-de99-47fa-84b9-4d0cdcbb93ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload an image from local storage\n",
    "uploader = widgets.FileUpload(accept='image/*', multiple=False)\n",
    "display(uploader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac069364-4014-48e6-826c-6d0de8dab588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the uploaded file\n",
    "image_data = uploader.value[0].content\n",
    "img_example = Image.open(BytesIO(image_data))\n",
    "\n",
    "# Create embeddings for the Image\n",
    "img_example_emb = embed_image(preprocess_it, model_it, img_example)\n",
    "img_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a1e0cb-074d-43c8-9127-586c633850c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Example: query by another game's cover or snapshot\n",
    "q_vec = embed_image(preprocess_it, model_it, img_example)\n",
    "\n",
    "res = collection_texts_images.query(\n",
    "    query_embeddings=[q_vec],\n",
    "    n_results=2000,\n",
    "    include=[\"documents\",\"distances\"]\n",
    ")\n",
    "\n",
    "print_top_k_files(res, k = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2416bf47-4205-4df1-8fd0-1a5ae85e4d48",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Using **clip4clip-webvid150k**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c093fce2-cb26-4046-9ce5-9615263a59e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer to project 512-d embeddings to 768-d\n",
    "projection = nn.Linear(512, 768).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946e7dab-9e84-44f2-b993-b5b279c9bea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_file = \"temp_video_in.mp4\"\n",
    "# We can use the following function to retrieve a video from our MinIO database\n",
    "# We are only extracting frames of the video, ignoring the audio content of the video\n",
    "def get_video(bucket = None, key = None, max_frames = 16, url = None):\n",
    "    if url:\n",
    "        r = requests.get(url, stream=True)\n",
    "        r.raise_for_status()\n",
    "        with open(temp_file, \"wb\") as f:\n",
    "            f.write(r.content)\n",
    "    else:\n",
    "        resp = s3.get_object(Bucket=bucket, Key=key)\n",
    "        body = resp[\"Body\"].read()\n",
    "        with open(temp_file, \"wb\") as f:\n",
    "            f.write(body)\n",
    "    frames = []\n",
    "    reader = imageio.get_reader(temp_file, format=\"ffmpeg\")\n",
    "    total_frames = reader.count_frames()\n",
    "    if total_frames and total_frames > 0:\n",
    "        step = max(1, total_frames // max_frames)\n",
    "        #print(total_frames, step, max_frames)\n",
    "        idxs = list(range(0, total_frames, step))[:max_frames]\n",
    "        for i in idxs:\n",
    "            try:\n",
    "                frame = reader.get_data(i)\n",
    "                frames.append(Image.fromarray(frame))\n",
    "            except Exception:\n",
    "                continue\n",
    "    else:\n",
    "        # fallback: iterate and collect up to max_frames\n",
    "        for i, frame in enumerate(reader):\n",
    "            frames.append(Image.fromarray(frame))\n",
    "            if len(frames) >= max_frames:\n",
    "                break\n",
    "    reader.close()\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61afbab5-1a51-4e6f-9fbc-aba9a2071dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def embed_image_second_model(processor, model, pil_img):\n",
    "    inputs = processor(images=pil_img, return_tensors=\"pt\").to(device)\n",
    "    outputs = model(**inputs)\n",
    "    feats = outputs.image_embeds\n",
    "    feats = feats / feats.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    # Project to 768-d\n",
    "    feats_768 = projection(feats)\n",
    "    feats_768 = feats_768 / feats_768.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    return feats_768.cpu().numpy().squeeze()\n",
    "\n",
    "@torch.no_grad()\n",
    "# The next function returns the embedding of the given text\n",
    "def embed_text_second_model(model, text: str, tokenizer=text_tokenizer):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=77).to(device) # max_length = 77 -> CLIP's max token length\n",
    "    outputs = model(**inputs)\n",
    "    text_features = outputs.text_embeds\n",
    "    text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    # Project to 768-d\n",
    "    text_features_768 = projection(text_features)\n",
    "    text_features_768 = text_features_768 / text_features_768.norm(dim=-1, keepdim=True)\n",
    "    return text_features_768.cpu().numpy()[0]\n",
    "\n",
    "# The next function returns the embedding of the given video\n",
    "def embed_video(tokenizer, model, frames_video):\n",
    "    inputs = tokenizer(images=frames_video, return_tensors=\"pt\")\n",
    "    pixel_values = inputs[\"pixel_values\"].to(device) # shape (num_frames, 3, H, W)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(pixel_values=pixel_values)\n",
    "        # prefer pooler_output if available, else mean over last_hidden_state\n",
    "        emb_frames = getattr(outputs, \"pooler_output\", None)\n",
    "        if emb_frames is None:\n",
    "            emb_frames = outputs.last_hidden_state.mean(dim=1)\n",
    "        # average frame embeddings to make video embedding\n",
    "        video_emb = emb_frames.mean(dim=0).cpu().numpy()\n",
    "    return  video_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1784cf0f-dc7f-41a4-a98e-d56560778218",
   "metadata": {},
   "source": [
    "**Text-based retrieval**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49bbd46-0065-4ac9-ad37-4d2dbb40acf4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Example: query by another game's description\n",
    "query_text = \"Games similar to Nier: Automata\"\n",
    "q_vec = embed_text_second_model(text_model, query_text).tolist()\n",
    "\n",
    "res = collection_texts_images_videos.query(\n",
    "    query_embeddings=[q_vec],\n",
    "    # It’s expected that all nearest neighbors are text for a long text query.\n",
    "    # To get images, we need to retrieve more embeddings.\n",
    "    n_results=2000,\n",
    "    include=[\"documents\",\"distances\"]\n",
    ")\n",
    "\n",
    "print_top_k_files(res, k = 5, videos = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7530b8-0ee5-4c0d-8f07-72071fb3c357",
   "metadata": {},
   "source": [
    "**Image-based retrieval**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee60f1bc-086b-4343-9ae2-ea3141489e9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Example: query by another game's cover or snapshot (using the same photo as the one uploaded previously)\n",
    "q_vec = embed_image_second_model(image_processor, vision_model, img_example)\n",
    "\n",
    "res = collection_texts_images_videos.query(\n",
    "    query_embeddings=[q_vec],\n",
    "    n_results=2000,\n",
    "    include=[\"documents\",\"distances\"]\n",
    ")\n",
    "\n",
    "print_top_k_files(res, k = 5,  videos = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985262e1-1694-4279-8ce5-ab35b60286bd",
   "metadata": {},
   "source": [
    "**Video-based retrieval**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32aaf8c9-509b-4e2e-a596-2e52b48bab8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample video\n",
    "video_example = get_video(url = \"https://cdn.akamai.steamstatic.com/steam/apps/256853884/movie_max.mp4?t=1633085092\") # frames/list of images of the video\n",
    "# Create embeddings for the video\n",
    "video_example_emb = embed_video(image_processor, vision_model, video_example)\n",
    "for frame in video_example:\n",
    "    display(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a6adef-c4a8-4331-b361-7366313b1007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: query by a list of frames from a game trailer\n",
    "res = collection_texts_images_videos.query(\n",
    "    query_embeddings=[video_example_emb],\n",
    "    n_results=2000,\n",
    "    include=[\"documents\",\"distances\"]\n",
    ")\n",
    "\n",
    "print_top_k_files(res, k = 5,  videos = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
