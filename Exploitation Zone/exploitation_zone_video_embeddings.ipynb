{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "960e9c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing useful dependencies\n",
    "import io\n",
    "import os\n",
    "import boto3\n",
    "import chromadb\n",
    "import torch\n",
    "from transformers import CLIPVisionModelWithProjection, CLIPImageProcessor\n",
    "from PIL import Image\n",
    "import imageio\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a4f9a351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup S3 client for MinIO (MinIO implements Amazon S3 API)\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    endpoint_url=\"http://127.0.0.1:9000\", # MinIO API endpoint\n",
    "    aws_access_key_id=\"minioadmin\", # User name\n",
    "    aws_secret_access_key=\"minioadmin\", # Password\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a46d04ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the server (Docker Container)\n",
    "client = chromadb.HttpClient(host=\"localhost\", port=8000)\n",
    "# Although we set a path for persistent directory when defining the Docker Container\n",
    "# It actually stores the embeddings inside the container\n",
    "\n",
    "# We can use the following line to remove all the stored data in a collection\n",
    "#client.delete_collection(name=\"texts\")\n",
    "\n",
    "# Create or get the collection named \"texts\"\n",
    "collection = client.create_collection(name=\"videos\", get_or_create=True, embedding_function=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7641f00d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPVisionModelWithProjection(\n",
       "  (vision_model): CLIPVisionTransformer(\n",
       "    (embeddings): CLIPVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "      (position_embedding): Embedding(50, 768)\n",
       "    )\n",
       "    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_name = \"Searchium-ai/clip4clip-webvid150k\"\n",
    "tokenizer = CLIPImageProcessor.from_pretrained(model_name)\n",
    "model = CLIPVisionModelWithProjection.from_pretrained(model_name)\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "af0c785a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "temp_file = \"temp_video_in.mp4\"\n",
    "def get_video(bucket, key, max_frames = 16):\n",
    "    resp = s3.get_object(Bucket=bucket, Key=key)\n",
    "    body = resp[\"Body\"].read()\n",
    "    with open(temp_file, \"wb\") as f:\n",
    "        f.write(body)\n",
    "    frames = []\n",
    "    reader = imageio.get_reader(temp_file, format=\"ffmpeg\")\n",
    "    total_frames = reader.count_frames()\n",
    "    if total_frames and total_frames > 0:\n",
    "        step = max(1, total_frames // max_frames)\n",
    "        print(total_frames, step, max_frames)\n",
    "        idxs = list(range(0, total_frames, step))[:max_frames]\n",
    "        for i in idxs:\n",
    "            try:\n",
    "                frame = reader.get_data(i)\n",
    "                frames.append(Image.fromarray(frame))\n",
    "            except Exception:\n",
    "                continue\n",
    "    else:\n",
    "        # fallback: iterate and collect up to max_frames\n",
    "        for i, frame in enumerate(reader):\n",
    "            frames.append(Image.fromarray(frame))\n",
    "            if len(frames) >= max_frames:\n",
    "                break\n",
    "    reader.close()\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "caa96c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def videos_to_embeddings(bucket, prefix, collection):\n",
    "    id_counter = 0\n",
    "\n",
    "    paginator = s3.get_paginator(\"list_objects_v2\")\n",
    "    for page in paginator.paginate(Bucket=bucket, Prefix=prefix):\n",
    "        paths = []\n",
    "        embeddings = []\n",
    "        ids = []\n",
    "\n",
    "        for obj in page.get(\"Contents\", []):\n",
    "            key = obj[\"Key\"]\n",
    "\n",
    "            if obj['Size'] == 0 and key.endswith(\"/\"):\n",
    "                continue\n",
    "\n",
    "            id_counter += 1\n",
    "\n",
    "            frames = get_video(bucket=bucket, key=key)\n",
    "            if not frames:\n",
    "                continue\n",
    "\n",
    "            inputs = tokenizer(images=frames, return_tensors=\"pt\")\n",
    "            pixel_values = inputs[\"pixel_values\"].to(device)  # shape (num_frames, 3, H, W)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(pixel_values=pixel_values)\n",
    "                # prefer pooler_output if available, else mean over last_hidden_state\n",
    "                emb_frames = getattr(outputs, \"pooler_output\", None)\n",
    "                if emb_frames is None:\n",
    "                    emb_frames = outputs.last_hidden_state.mean(dim=1)\n",
    "                # average frame embeddings to make video embedding\n",
    "                video_emb = emb_frames.mean(dim=0).cpu().numpy()\n",
    "                # normalize\n",
    "                norm = np.linalg.norm(video_emb)\n",
    "                if norm > 0:\n",
    "                    video_emb = video_emb / norm\n",
    "\n",
    "            paths.append(f\"{bucket}/{key}\")\n",
    "            embeddings.append(video_emb.tolist())\n",
    "            ids.append(f\"vid_{id_counter}\")\n",
    "\n",
    "            print(f\"Created embedding for {key} ({len(embeddings)} items in current batch).\")\n",
    "\n",
    "        # upsert into chroma collection\n",
    "        collection.add(\n",
    "            ids=ids,\n",
    "            documents=paths,\n",
    "            embeddings=embeddings\n",
    "        )\n",
    "        \n",
    "        print(f\"All embeddings in the current batch are store successfully in the collection {collection.name}.\")\n",
    "    os.remove(temp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1d385e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3582 223 16\n",
      "Created embedding for videos/video_1758900500520.mp4 (1 items in current batch).\n",
      "1980 123 16\n",
      "Created embedding for videos/video_1758900500636.mp4 (2 items in current batch).\n",
      "1886 117 16\n",
      "Created embedding for videos/video_1758900500710.mp4 (3 items in current batch).\n",
      "All embeddings in the current batch are store successfully in the collection videos.\n"
     ]
    }
   ],
   "source": [
    "videos_to_embeddings(bucket = \"trusted-zone\", prefix = \"videos/\", collection = collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f7f77cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: vid_1\n",
      "Document: trusted-zone/videos/video_1758900500520.mp4\n",
      "Embedding (first 5 dims): [ 0.00670266  0.03183165 -0.01030848 -0.04513294  0.01144601]\n",
      "---\n",
      "ID: vid_2\n",
      "Document: trusted-zone/videos/video_1758900500636.mp4\n",
      "Embedding (first 5 dims): [-0.00259666  0.03752612 -0.03215834 -0.03951677  0.01091534]\n",
      "---\n",
      "ID: vid_3\n",
      "Document: trusted-zone/videos/video_1758900500710.mp4\n",
      "Embedding (first 5 dims): [ 0.00654727  0.03116178 -0.0128628  -0.04546758  0.00775671]\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "def print_stored_embeddings(collection, x=None): # x is the maximum number of files to print\n",
    "    results = collection.get(include=[\"documents\", \"embeddings\"])\n",
    "    for i in range(len(results[\"documents\"])):\n",
    "        print(\"ID:\", results['ids'][i])\n",
    "        print(\"Document:\", results[\"documents\"][i])\n",
    "        print(\"Embedding (first 5 dims):\", results[\"embeddings\"][i][:5])\n",
    "        print(\"---\")\n",
    "        if x and (x-1) == i:\n",
    "            break\n",
    "        \n",
    "# We can use this function to print the embeddings stored in chromaDB\n",
    "print_stored_embeddings(collection, x = 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
