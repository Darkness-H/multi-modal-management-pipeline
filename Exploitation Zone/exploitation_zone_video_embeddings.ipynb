{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3a7a52e-69c8-4c26-8333-ef1dfd536dd8",
   "metadata": {},
   "source": [
    "## Video Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "960e9c71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.1.3 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 607, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 1919, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 519, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 508, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 400, in dispatch_shell\n",
      "    await result\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 368, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 455, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 577, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3116, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3171, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3394, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3639, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3699, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Temp\\ipykernel_21132\\4052611638.py\", line 7, in <module>\n",
      "    from transformers import CLIPVisionModelWithProjection, CLIPImageProcessor\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\import_utils.py\", line 2317, in __getattr__\n",
      "    module = self._get_module(self._class_to_module[name])\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\import_utils.py\", line 2345, in _get_module\n",
      "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\importlib\\__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\clip\\modeling_clip.py\", line 25, in <module>\n",
      "    from ...modeling_layers import GradientCheckpointingLayer\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\modeling_layers.py\", line 27, in <module>\n",
      "    from .models.auto import AutoModel\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\import_utils.py\", line 2317, in __getattr__\n",
      "    module = self._get_module(self._class_to_module[name])\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\import_utils.py\", line 2345, in _get_module\n",
      "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\importlib\\__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py\", line 23, in <module>\n",
      "    from .auto_factory import (\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 43, in <module>\n",
      "    from ...generation import GenerationMixin\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\import_utils.py\", line 2317, in __getattr__\n",
      "    module = self._get_module(self._class_to_module[name])\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\import_utils.py\", line 2345, in _get_module\n",
      "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\importlib\\__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py\", line 55, in <module>\n",
      "    from .candidate_generator import (\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\candidate_generator.py\", line 29, in <module>\n",
      "    from sklearn.metrics import roc_curve\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\__init__.py\", line 73, in <module>\n",
      "    from .base import clone  # noqa: E402\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 19, in <module>\n",
      "    from .utils._metadata_requests import _MetadataRequester, _routing_enabled\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\__init__.py\", line 9, in <module>\n",
      "    from ._chunking import gen_batches, gen_even_slices\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_chunking.py\", line 11, in <module>\n",
      "    from ._param_validation import Interval, validate_params\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 17, in <module>\n",
      "    from .validation import _is_arraylike_not_scalar\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 21, in <module>\n",
      "    from ..utils._array_api import _asarray_with_order, _is_numpy_namespace, get_namespace\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_array_api.py\", line 20, in <module>\n",
      "    from .fixes import parse_version\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\fixes.py\", line 20, in <module>\n",
      "    import pandas as pd\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\__init__.py\", line 61, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\api.py\", line 28, in <module>\n",
      "    from pandas.core.arrays import Categorical\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\arrays\\__init__.py\", line 1, in <module>\n",
      "    from pandas.core.arrays.arrow import ArrowExtensionArray\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\arrays\\arrow\\__init__.py\", line 5, in <module>\n",
      "    from pandas.core.arrays.arrow.array import ArrowExtensionArray\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\arrays\\arrow\\array.py\", line 52, in <module>\n",
      "    from pandas.core import (\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\ops\\__init__.py\", line 8, in <module>\n",
      "    from pandas.core.ops.array_ops import (\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\ops\\array_ops.py\", line 56, in <module>\n",
      "    from pandas.core.computation import expressions\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\computation\\expressions.py\", line 21, in <module>\n",
      "    from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\computation\\check.py\", line 5, in <module>\n",
      "    ne = import_optional_dependency(\"numexpr\", errors=\"warn\")\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\compat\\_optional.py\", line 135, in import_optional_dependency\n",
      "    module = importlib.import_module(name)\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\importlib\\__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numexpr\\__init__.py\", line 24, in <module>\n",
      "    from numexpr.interpreter import MAX_THREADS, use_vml, __BLOCK_SIZE1__\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[31mAttributeError\u001b[39m: _ARRAY_API not found"
     ]
    }
   ],
   "source": [
    "# Importing useful dependencies\n",
    "import io\n",
    "import os\n",
    "import boto3\n",
    "import chromadb\n",
    "import torch\n",
    "from transformers import CLIPVisionModelWithProjection, CLIPImageProcessor\n",
    "from PIL import Image\n",
    "import imageio\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4f9a351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup S3 client for MinIO (MinIO implements Amazon S3 API)\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    endpoint_url=\"http://127.0.0.1:9000\", # MinIO API endpoint\n",
    "    aws_access_key_id=\"minioadmin\", # User name\n",
    "    aws_secret_access_key=\"minioadmin\", # Password\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a46d04ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the server (Docker Container)\n",
    "client = chromadb.HttpClient(host=\"localhost\", port=8000)\n",
    "# Although we set a path for persistent directory when defining the Docker Container\n",
    "# It actually stores the embeddings inside the container\n",
    "\n",
    "# We can use the following line to remove all the stored data in a collection\n",
    "#client.delete_collection(name=\"texts\")\n",
    "\n",
    "# Create or get the collection named \"texts\"\n",
    "collection = client.create_collection(name=\"videos\", get_or_create=True, embedding_function=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7641f00d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e05c9f57ca764572a2db14bd40e205f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SakuraSnow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\SakuraSnow\\.cache\\huggingface\\hub\\models--Searchium-ai--clip4clip-webvid150k. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd9529916947494d9e83dc9ec75dcb09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5308743d80064e5ebb6bb68221ad0a4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/605M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CLIPVisionModelWithProjection(\n",
       "  (vision_model): CLIPVisionTransformer(\n",
       "    (embeddings): CLIPVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "      (position_embedding): Embedding(50, 768)\n",
       "    )\n",
       "    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_name = \"Searchium-ai/clip4clip-webvid150k\"\n",
    "tokenizer = CLIPImageProcessor.from_pretrained(model_name)\n",
    "model = CLIPVisionModelWithProjection.from_pretrained(model_name)\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af0c785a",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_file = \"temp_video_in.mp4\"\n",
    "def get_video(bucket, key, max_frames = 16):\n",
    "    resp = s3.get_object(Bucket=bucket, Key=key)\n",
    "    body = resp[\"Body\"].read()\n",
    "    with open(temp_file, \"wb\") as f:\n",
    "        f.write(body)\n",
    "    frames = []\n",
    "    reader = imageio.get_reader(temp_file, format=\"ffmpeg\")\n",
    "    total_frames = reader.count_frames()\n",
    "    if total_frames and total_frames > 0:\n",
    "        step = max(1, total_frames // max_frames)\n",
    "        #print(total_frames, step, max_frames)\n",
    "        idxs = list(range(0, total_frames, step))[:max_frames]\n",
    "        for i in idxs:\n",
    "            try:\n",
    "                frame = reader.get_data(i)\n",
    "                frames.append(Image.fromarray(frame))\n",
    "            except Exception:\n",
    "                continue\n",
    "    else:\n",
    "        # fallback: iterate and collect up to max_frames\n",
    "        for i, frame in enumerate(reader):\n",
    "            frames.append(Image.fromarray(frame))\n",
    "            if len(frames) >= max_frames:\n",
    "                break\n",
    "    reader.close()\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "caa96c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def videos_to_embeddings(bucket, prefix, collection):\n",
    "    id_counter = 0\n",
    "\n",
    "    paginator = s3.get_paginator(\"list_objects_v2\")\n",
    "    for page in paginator.paginate(Bucket=bucket, Prefix=prefix):\n",
    "        paths = []\n",
    "        embeddings = []\n",
    "        ids = []\n",
    "\n",
    "        for obj in page.get(\"Contents\", []):\n",
    "            key = obj[\"Key\"]\n",
    "\n",
    "            if obj['Size'] == 0 and key.endswith(\"/\"):\n",
    "                continue\n",
    "\n",
    "            id_counter += 1\n",
    "\n",
    "            frames = get_video(bucket=bucket, key=key)\n",
    "            if not frames:\n",
    "                continue\n",
    "\n",
    "            inputs = tokenizer(images=frames, return_tensors=\"pt\")\n",
    "            pixel_values = inputs[\"pixel_values\"].to(device)  # shape (num_frames, 3, H, W)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(pixel_values=pixel_values)\n",
    "                # prefer pooler_output if available, else mean over last_hidden_state\n",
    "                emb_frames = getattr(outputs, \"pooler_output\", None)\n",
    "                if emb_frames is None:\n",
    "                    emb_frames = outputs.last_hidden_state.mean(dim=1)\n",
    "                # average frame embeddings to make video embedding\n",
    "                video_emb = emb_frames.mean(dim=0).cpu().numpy()\n",
    "                # normalize\n",
    "                norm = np.linalg.norm(video_emb)\n",
    "                if norm > 0:\n",
    "                    video_emb = video_emb / norm\n",
    "\n",
    "            paths.append(f\"{bucket}/{key}\")\n",
    "            embeddings.append(video_emb.tolist())\n",
    "            ids.append(f\"vid_{id_counter}\")\n",
    "\n",
    "            print(f\"Created embedding for {key} ({len(embeddings)} items in current batch).\")\n",
    "\n",
    "        # upsert into chroma collection\n",
    "        collection.add(\n",
    "            ids=ids,\n",
    "            documents=paths,\n",
    "            embeddings=embeddings\n",
    "        )\n",
    "        \n",
    "        print(f\"All embeddings in the current batch are store successfully in the collection {collection.name}.\")\n",
    "    os.remove(temp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d385e37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3582 223 16\n",
      "Created embedding for videos/video_1760786503181.mp4 (1 items in current batch).\n",
      "6059 378 16\n",
      "Created embedding for videos/video_1760786503789.mp4 (2 items in current batch).\n",
      "4800 300 16\n",
      "Created embedding for videos/video_1760786504973.mp4 (3 items in current batch).\n",
      "1517 94 16\n",
      "Created embedding for videos/video_1760786506567.mp4 (4 items in current batch).\n",
      "1649 103 16\n",
      "Created embedding for videos/video_1760786507389.mp4 (5 items in current batch).\n",
      "3224 201 16\n",
      "Created embedding for videos/video_1760786507749.mp4 (6 items in current batch).\n",
      "5845 365 16\n",
      "Created embedding for videos/video_1760786509055.mp4 (7 items in current batch).\n",
      "1772 110 16\n",
      "Created embedding for videos/video_1760786510257.mp4 (8 items in current batch).\n",
      "1800 112 16\n",
      "Created embedding for videos/video_1760786511011.mp4 (9 items in current batch).\n",
      "2285 142 16\n",
      "Created embedding for videos/video_1760786511692.mp4 (10 items in current batch).\n",
      "1980 123 16\n",
      "Created embedding for videos/video_1760786512108.mp4 (11 items in current batch).\n",
      "2354 147 16\n",
      "Created embedding for videos/video_1760786512480.mp4 (12 items in current batch).\n",
      "653 40 16\n",
      "Created embedding for videos/video_1760786513139.mp4 (13 items in current batch).\n",
      "1286 80 16\n",
      "Created embedding for videos/video_1760786513515.mp4 (14 items in current batch).\n",
      "3070 191 16\n",
      "Created embedding for videos/video_1760786514050.mp4 (15 items in current batch).\n",
      "5667 354 16\n",
      "Created embedding for videos/video_1760786515094.mp4 (16 items in current batch).\n",
      "7200 450 16\n",
      "Created embedding for videos/video_1760786516115.mp4 (17 items in current batch).\n",
      "2100 131 16\n",
      "Created embedding for videos/video_1760786518135.mp4 (18 items in current batch).\n",
      "877 54 16\n",
      "Created embedding for videos/video_1760786519163.mp4 (19 items in current batch).\n",
      "1663 103 16\n",
      "Created embedding for videos/video_1760786519505.mp4 (20 items in current batch).\n",
      "1844 115 16\n",
      "Created embedding for videos/video_1760786520425.mp4 (21 items in current batch).\n",
      "1886 117 16\n",
      "Created embedding for videos/video_1760786521389.mp4 (22 items in current batch).\n",
      "2750 171 16\n",
      "Created embedding for videos/video_1760786521927.mp4 (23 items in current batch).\n",
      "816 51 16\n",
      "Created embedding for videos/video_1760786523620.mp4 (24 items in current batch).\n",
      "4205 262 16\n",
      "Created embedding for videos/video_1760786524141.mp4 (25 items in current batch).\n",
      "1309 81 16\n",
      "Created embedding for videos/video_1760786524944.mp4 (26 items in current batch).\n",
      "4230 264 16\n",
      "Created embedding for videos/video_1760786525228.mp4 (27 items in current batch).\n",
      "10930 683 16\n",
      "Created embedding for videos/video_1760786527211.mp4 (28 items in current batch).\n",
      "1109 69 16\n",
      "Created embedding for videos/video_1760786528313.mp4 (29 items in current batch).\n",
      "All embeddings in the current batch are store successfully in the collection videos.\n"
     ]
    }
   ],
   "source": [
    "videos_to_embeddings(bucket = \"trusted-zone\", prefix = \"videos/\", collection = collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7f77cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: vid_1\n",
      "Document: trusted-zone/videos/video_1760786503181.mp4\n",
      "Embedding (first 5 dims): [ 0.00699654  0.03213104 -0.01002311 -0.04518802  0.0115661 ]\n",
      "---\n",
      "ID: vid_2\n",
      "Document: trusted-zone/videos/video_1760786503789.mp4\n",
      "Embedding (first 5 dims): [ 0.01091814  0.04181465 -0.02120226 -0.04388113 -0.00216616]\n",
      "---\n",
      "ID: vid_3\n",
      "Document: trusted-zone/videos/video_1760786504973.mp4\n",
      "Embedding (first 5 dims): [ 0.01047543  0.03237706 -0.01970521 -0.04911201  0.00637116]\n",
      "---\n",
      "ID: vid_4\n",
      "Document: trusted-zone/videos/video_1760786506567.mp4\n",
      "Embedding (first 5 dims): [ 0.00626602  0.03580121 -0.01199441 -0.04718189  0.01287883]\n",
      "---\n",
      "ID: vid_5\n",
      "Document: trusted-zone/videos/video_1760786507389.mp4\n",
      "Embedding (first 5 dims): [ 0.00306915  0.02716548 -0.02542129 -0.04034228  0.0046057 ]\n",
      "---\n",
      "ID: vid_6\n",
      "Document: trusted-zone/videos/video_1760786507749.mp4\n",
      "Embedding (first 5 dims): [ 0.00736225  0.03909471 -0.00081623 -0.04899862  0.00935894]\n",
      "---\n",
      "ID: vid_7\n",
      "Document: trusted-zone/videos/video_1760786509055.mp4\n",
      "Embedding (first 5 dims): [ 0.01101903  0.04182591 -0.01180492 -0.04202618  0.00557751]\n",
      "---\n",
      "ID: vid_8\n",
      "Document: trusted-zone/videos/video_1760786510257.mp4\n",
      "Embedding (first 5 dims): [ 0.00559662  0.03917652 -0.02019663 -0.0421516   0.01441046]\n",
      "---\n",
      "ID: vid_9\n",
      "Document: trusted-zone/videos/video_1760786511011.mp4\n",
      "Embedding (first 5 dims): [ 0.01369758  0.03803894 -0.02208896 -0.04527741  0.00417117]\n",
      "---\n",
      "ID: vid_10\n",
      "Document: trusted-zone/videos/video_1760786511692.mp4\n",
      "Embedding (first 5 dims): [ 0.0013092   0.03020277 -0.00304495 -0.05745922  0.00341662]\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "def print_stored_embeddings(collection, x=None): # x is the maximum number of files to print\n",
    "    results = collection.get(include=[\"documents\", \"embeddings\"])\n",
    "    for i in range(len(results[\"documents\"])):\n",
    "        print(\"ID:\", results['ids'][i])\n",
    "        print(\"Document:\", results[\"documents\"][i])\n",
    "        print(\"Embedding (first 5 dims):\", results[\"embeddings\"][i][:5])\n",
    "        print(\"---\")\n",
    "        if x and (x-1) == i:\n",
    "            break\n",
    "        \n",
    "# We can use this function to print the embeddings stored in chromaDB\n",
    "print_stored_embeddings(collection, x = 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
