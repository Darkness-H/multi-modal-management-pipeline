{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5492709b-7c5d-4280-827c-0686594cd526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing useful dependencies\n",
    "import io\n",
    "import os\n",
    "import boto3\n",
    "import torch\n",
    "import chromadb\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import numpy as np\n",
    "from chromadb.config import Settings\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca15c09-eaa6-420f-a272-dd6de5c54360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup S3 client for MinIO (MinIO implements Amazon S3 API)\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    endpoint_url=\"http://127.0.0.1:9000\", # MinIO API endpoint\n",
    "    aws_access_key_id=\"minioadmin\", # User name\n",
    "    aws_secret_access_key=\"minioadmin\", # Password\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf61fe72-edae-467f-90ab-fc7766330efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the server (Docker Container)\n",
    "client = chromadb.HttpClient(host=\"localhost\", port=8000)\n",
    "# Although we set a path for persistent directory when defining the Docker Container\n",
    "# It actually stores the embeddings inside the container\n",
    "\n",
    "# We can use the following line to remove all the stored data in a collection\n",
    "#client.delete_collection(name=\"images\")\n",
    "\n",
    "# Create or get the collection named \"images\"\n",
    "collection = client.create_collection(name=\"texts\", get_or_create=True, embedding_function=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd1e475-13bd-42bd-95b5-b2133134c449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just in case our device has gpu\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-cased')\n",
    "model = BertModel.from_pretrained(\"bert-large-cased\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "301982be-30e1-40d8-b29b-daa7b67bcca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use this function to retrieve an text from our bucket\n",
    "def get_text(bucket, key):\n",
    "    resp = s3.get_object(Bucket=bucket, Key=key)\n",
    "    body = resp[\"Body\"].read()\n",
    "    text = body.decode(\"utf-8\")\n",
    "    return text\n",
    "# The next function returns the embedding of the given text\n",
    "def embed_text(tokenizer, model, text):\n",
    "    encoded_input = tokenizer(text, return_tensors='pt',truncation=True,max_length=512)\n",
    "    with torch.no_grad():\n",
    "        output = model(**encoded_input)\n",
    "        feats = output.pooler_output\n",
    "        feats = feats / feats.norm(dim=-1, keepdim=True)\n",
    "        feats_np = feats.cpu().numpy().squeeze()\n",
    "    return  feats_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0272af9d-9d82-4432-bf19-b67ff1fae563",
   "metadata": {},
   "outputs": [],
   "source": [
    "def texts_to_embeddings(src_bucket, collection,tokenizer , model, src_prefix=\"\"):\n",
    "\n",
    "    # Incremental id assigned to each image embedding\n",
    "    id_counter = 0\n",
    "    \n",
    "    paginator = s3.get_paginator(\"list_objects_v2\") # It returns objects in pages and not all at once.\n",
    "    for page in paginator.paginate(Bucket=src_bucket, Prefix=src_prefix):\n",
    "\n",
    "        # List of paths (meta_data)\n",
    "        texts_paths = []\n",
    "        # List of embeddings\n",
    "        embeddings = []\n",
    "        # List of unique IDs for each embedding\n",
    "        ids = []\n",
    "        \n",
    "        for obj in page.get(\"Contents\", []):\n",
    "\n",
    "            key = obj[\"Key\"]\n",
    "\n",
    "            if obj['Size'] == 0 and key.endswith(\"/\"): # skip the folder itself\n",
    "                continue\n",
    "\n",
    "            id_counter += 1\n",
    "\n",
    "            # Download the image\n",
    "            text = get_text(src_bucket, key)\n",
    "            \n",
    "            # Compute embedding\n",
    "            vector = embed_text(tokenizer, model, text) # A numerical vector of size 1024\n",
    "\n",
    "            print(f\"Created embedding for {key} ({len(embeddings)} items in current batch).\")\n",
    "\n",
    "            # Storing data\n",
    "            texts_paths.append(f\"{src_bucket}/{key}\")\n",
    "            embeddings.append(vector)\n",
    "            ids.append(f\"text_{id_counter}\")\n",
    "\n",
    "        # Store the images of a page at once\n",
    "        collection.add(\n",
    "                ids=ids,\n",
    "                documents=texts_paths,\n",
    "                embeddings=embeddings\n",
    "        )\n",
    "\n",
    "        print(f\"All embeddings in the current batch are store successfully in the collection {collection.name}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870c18c1-d4b8-44e5-8035-4c80ac6d91e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_to_embeddings(src_bucket = \"trusted-zone\", src_prefix = \"texts/\", collection = collection, tokenizer = tokenizer, model = model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfab65f-eac3-4e91-9557-90b765bcbd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that prints the embeddings stored in a collection\n",
    "def print_stored_embeddings(collection, x=None): # x is the maximum number of files to print\n",
    "    results = collection.get(include=[\"documents\", \"embeddings\"])\n",
    "    for i in range(len(results[\"documents\"])):\n",
    "        print(\"ID:\", results['ids'][i])\n",
    "        print(\"Document:\", results[\"documents\"][i])\n",
    "        print(\"Embedding (first 5 dims):\", results[\"embeddings\"][i][:5])\n",
    "        print(\"---\")\n",
    "        if x and (x-1) == i:\n",
    "            break\n",
    "\n",
    "# We can use this function to print the embeddings stored in chromaDB\n",
    "print_stored_embeddings(collection, x = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "64ded665-a598-41f3-bcce-574cdef17a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now perform a similarity search to test it\n",
    "\n",
    "# The following function searches the top k most similar images in ChromaDB using the embeddings of an text\n",
    "def find_similar_texts(collection, query_emb: np.ndarray, top_k: int = 5):\n",
    "    # Chroma expects list-of-lists for query_embeddings\n",
    "    query_vector = query_emb.tolist()\n",
    "\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_vector],\n",
    "        n_results=top_k,\n",
    "        include=[\"documents\", \"distances\"]\n",
    "    )\n",
    "\n",
    "    # Extract first query results\n",
    "    ids = results.get(\"ids\", [[]])[0]\n",
    "    docs = results.get(\"documents\", [[]])[0]\n",
    "    dists = results.get(\"distances\", [[]])[0]\n",
    "\n",
    "    print(f\"Top {top_k} similar texts:\")\n",
    "    for rank, (doc_id, doc, dist) in enumerate(zip(ids, docs, dists), start=1):\n",
    "        similarity = 1 - dist  \n",
    "        print(f\"{rank}. id={doc_id}, distance={dist:.4f} (similarity={similarity:.4f})\")\n",
    "        print(f\"   text: {doc[:200]}{'...' if len(doc) > 200 else ''}\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4d4286fd-7810-4acc-8dea-d2cd1a743ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 similar texts:\n",
      "1. id=img_1, distance=0.0000 (similarity=1.0000)\n",
      "   text: trusted-zone/texts/text_1760137269318.txt\n",
      "2. id=img_64, distance=0.0036 (similarity=0.9964)\n",
      "   text: trusted-zone/texts/text_1760137274219.txt\n",
      "3. id=img_160, distance=0.0041 (similarity=0.9959)\n",
      "   text: trusted-zone/texts/text_1760137281032.txt\n",
      "4. id=img_217, distance=0.0041 (similarity=0.9959)\n",
      "   text: trusted-zone/texts/text_1760137285646.txt\n",
      "5. id=img_106, distance=0.0044 (similarity=0.9956)\n",
      "   text: trusted-zone/texts/text_1760137277237.txt\n",
      "{'ids': [['img_1', 'img_64', 'img_160', 'img_217', 'img_106']], 'distances': [[0.0, 0.003573833, 0.0041247564, 0.004138467, 0.0043580746]], 'embeddings': None, 'metadatas': None, 'documents': [['trusted-zone/texts/text_1760137269318.txt', 'trusted-zone/texts/text_1760137274219.txt', 'trusted-zone/texts/text_1760137281032.txt', 'trusted-zone/texts/text_1760137285646.txt', 'trusted-zone/texts/text_1760137277237.txt']], 'uris': None, 'data': None, 'included': ['documents', 'distances']}\n"
     ]
    }
   ],
   "source": [
    "# Search for similar texts in ChromaDB\n",
    "results = find_similar_texts(collection, emb, top_k=5) # The first one is always the target texts itself\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "07a6e769-5f40-41c3-be5a-a5c2cbbfc252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. 📄 trusted-zone/texts/text_1760137269318.txt\n",
      "Galactic Bowling is an exaggerated and stylized bowling game with an intergalactic twist. Players will engage in fast-paced single and multi-player competition while being submerged in a unique new universe filled with over-the-top humor, wild characters, unique levels, and addictive game play. The  ...\n",
      "\n",
      "2. 📄 trusted-zone/texts/text_1760137274219.txt\n",
      "This ain't your Grandma's Tetris! Vetrix is a puzzle game inspired by Tetris, but with its own original mechanics built for virtual reality. The player will have to demonstrate dexterity, organization, and good skills in the geometry of space as well as proprioception to achieve the best possible sc ...\n",
      "\n",
      "3. 📄 trusted-zone/texts/text_1760137281032.txt\n",
      "At Cub Gym the player will have a huge challenge ahead, several crazy games are waiting for him trying to take him down, test his skills and make it to the end. With a completely random level creation system, Cub Gym promises different routes for each match, always trying to surprise the player by a ...\n",
      "\n",
      "4. 📄 trusted-zone/texts/text_1760137285646.txt\n",
      "Jolt Project: The army now has a new robotics project, jolt. It's up to you to control it and ensure the success of the missions! There are 9 stages of taking the breath away with the right difficulty and good gameplay. Plus an insane way of survival! Fire missiles at cars, tanks, helicopters and tu ...\n",
      "\n",
      "5. 📄 trusted-zone/texts/text_1760137277237.txt\n",
      "Tired of arguing on the internet over which rocket is best? Orbital Combat is the game created to settle those disputes! With a large selection of characters and arenas, you'll be able to create any scenario you'd like when it comes to fighting spacecrafts! 8 Characters to Play as or Fight Against!  ...\n"
     ]
    }
   ],
   "source": [
    "def get_query_texts(results, bucket=\"trusted-zone\"):\n",
    "    \"\"\"\n",
    "    给定 ChromaDB 的 query() 结果，返回对应的文本内容。\n",
    "    \"\"\"\n",
    "    docs = results.get(\"documents\", [[]])[0]  # 取第一个 query 的 documents 列表\n",
    "    texts = []\n",
    "\n",
    "    for doc_path in docs:\n",
    "        # 从路径中提取出 key，比如 'trusted-zone/texts/text_1760137269318.txt'\n",
    "        # 去掉前缀 'trusted-zone/' 部分\n",
    "        key = doc_path.split(\"trusted-zone/\")[-1]\n",
    "        try:\n",
    "            text = get_text(bucket, key)\n",
    "            texts.append({\"path\": doc_path, \"content\": text})\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Failed to read {key}: {e}\")\n",
    "            texts.append({\"path\": doc_path, \"content\": None})\n",
    "    return texts\n",
    "texts = get_query_texts(results)\n",
    "for i, t in enumerate(texts, start=1):\n",
    "    print(f\"\\n{i}. 📄 {t['path']}\")\n",
    "    print(t['content'][:300], \"...\" if len(t['content']) > 300 else \"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
