{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cefc238-8601-4418-a77c-10c2b7c198a3",
   "metadata": {},
   "source": [
    "## TextHomogenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d73a52fc-33d9-4dc7-a143-fbb988e28279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing useful dependencies\n",
    "import io\n",
    "import os\n",
    "import boto3\n",
    "import chardet\n",
    "import docx\n",
    "import fitz\n",
    "import zipfile\n",
    "from bs4 import BeautifulSoup\n",
    "from odf.opendocument import load\n",
    "from odf.text import P\n",
    "from odf.teletype import extractText as odf_text\n",
    "from striprtf.striprtf import rtf_to_text\n",
    "import re\n",
    "from markdown_it import MarkdownIt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5dde4a2-fe5f-45fd-84b4-35ee7ec4c89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup S3 client for MinIO (MinIO implements Amazon S3 API)\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    endpoint_url=\"http://127.0.0.1:9000\", # MinIO API endpoint\n",
    "    aws_access_key_id=\"minioadmin\", # User name\n",
    "    aws_secret_access_key=\"minioadmin\", # Password\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "87332a35-31f2-48df-82ad-9ca134bb89d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all function to extract text from other format\n",
    "\n",
    "\n",
    "# Helper: normalize paragraph spacing\n",
    "def _norm(s: str) -> str:\n",
    "    \"\"\"Normalize newlines and ensure paragraphs are separated by one blank line.\"\"\"\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    s = s.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\").strip()\n",
    "    # Collapse 3 or more newlines into 2 (blank line between paragraphs)\n",
    "    s = re.sub(r\"\\n{3,}\", \"\\n\\n\", s)\n",
    "    return s\n",
    "\n",
    "# DOCX\n",
    "def extract_from_docx(body: bytes) -> str:\n",
    "    \"\"\"Extract text from DOCX, preserving paragraphs.\"\"\"\n",
    "    doc = docx.Document(io.BytesIO(body))\n",
    "    return _norm(\"\\n\\n\".join(p.text.strip() for p in doc.paragraphs if p.text.strip()))\n",
    "\n",
    "# PDF (block-level extraction for better paragraph preservation)\n",
    "\n",
    "def extract_from_pdf(body: bytes) -> str:\n",
    "    \"\"\"Extract text from PDF by text blocks (more reliable paragraph grouping).\"\"\"\n",
    "    doc = fitz.open(stream=io.BytesIO(body), filetype=\"pdf\")\n",
    "    pages = []\n",
    "    for i in range(len(doc)):\n",
    "        blocks = doc.load_page(i).get_text(\"blocks\")\n",
    "        blocks.sort(key=lambda b: (b[1], b[0]))  # Sort by y, then x\n",
    "        txts = [b[4].strip() for b in blocks if b[4].strip()]\n",
    "        pages.append(\"\\n\\n\".join(txts))\n",
    "    return _norm(\"\\n\\n\".join(pages))\n",
    "\n",
    "# EPUB or ZIP (HTML)\n",
    "def extract_from_zip(epub_bytes: bytes) -> str:\n",
    "    \"\"\"Extract text from EPUB/ZIP containing HTML or XHTML files.\"\"\"\n",
    "    parts = []\n",
    "    with zipfile.ZipFile(io.BytesIO(epub_bytes)) as z:\n",
    "        for name in sorted(z.namelist()):\n",
    "            if name.lower().endswith((\".xhtml\", \".html\", \".htm\")):\n",
    "                with z.open(name) as f:\n",
    "                    soup = BeautifulSoup(f.read(), \"html.parser\")\n",
    "                    parts.append(soup.get_text(separator=\"\\n\").strip())\n",
    "    return _norm(\"\\n\\n\".join(parts))\n",
    "\n",
    "# HTML\n",
    "def extract_from_html(html_bytes: bytes) -> str:\n",
    "    \"\"\"Extract text from HTML while keeping line breaks between elements.\"\"\"\n",
    "    soup = BeautifulSoup(html_bytes, \"html.parser\")\n",
    "    return _norm(soup.get_text(separator=\"\\n\"))\n",
    "\n",
    "# ODT\n",
    "def extract_from_odt(odt_bytes: bytes) -> str:\n",
    "    \"\"\"Extract text from ODT file, preserving paragraphs.\"\"\"\n",
    "    doc = load(io.BytesIO(odt_bytes))\n",
    "    paras = [odf_text(p).strip() for p in doc.getElementsByType(P)]\n",
    "    return _norm(\"\\n\\n\".join(p for p in paras if p))\n",
    "\n",
    "# RTF\n",
    "def extract_from_rtf(rtf_bytes: bytes) -> str:\n",
    "    \"\"\"Extract text from RTF; prefer using striprtf if available.\"\"\"\n",
    "    try:\n",
    "        text = rtf_to_text(rtf_bytes.decode(\"utf-8\", errors=\"ignore\"))\n",
    "    except Exception:\n",
    "        # Fallback: remove control words and treat \\par as paragraph break\n",
    "        s = rtf_bytes.decode(\"utf-8\", errors=\"ignore\")\n",
    "        s = re.sub(r\"\\\\par[d]?\", \"\\n\\n\", s)\n",
    "        s = re.sub(r\"{\\\\[^}]*}\", \" \", s)\n",
    "        s = re.sub(r\"\\\\[a-zA-Z]+-?\\d*\", \" \", s)\n",
    "        s = re.sub(r\"[{}]\", \" \", s)\n",
    "        text = s\n",
    "    return _norm(text)\n",
    "\n",
    "# Markdown\n",
    "def extract_from_md(md_bytes: bytes) -> str:\n",
    "    \"\"\"\n",
    "    Markdown -> plain text, drop ALL images.\n",
    "    Handles:\n",
    "      - HTML images: <img ...>\n",
    "      - Inline images: ![alt](url \"title\")\n",
    "      - Reference images: ![alt][id]\n",
    "      - Reference definitions: [id]: <url or data:image...> (\"title\")\n",
    "      - Obsidian embeds: ![[file.png]]\n",
    "      - Long data-URI definitions that run to EOF (hard cut)\n",
    "    Ensures clean paragraph spacing.\n",
    "    \"\"\"\n",
    "    s = md_bytes.decode(\"utf-8\", errors=\"replace\")\n",
    "\n",
    "    # 0) Obsidian-style embeds: ![[image.png]]\n",
    "    s = re.sub(r\"!\\[\\[[^\\]]+\\]\\]\", \"\", s)\n",
    "\n",
    "    # 1) Remove HTML <img ...> (covers data:image in HTML as well)\n",
    "    s = re.sub(r\"(?is)<img\\b[^>]*>\", \"\", s)\n",
    "\n",
    "    # 2) Remove inline images: ![alt](url \"title\")\n",
    "    s = re.sub(\n",
    "        r\"!\\[[^\\]]*\\]\\(\\s*<?[^)\\s>]+[^)]*?>?\\s*(?:\\\"[^\\\"]*\\\"|'[^']*'|\\([^)]+\\))?\\s*\\)\",\n",
    "        \"\",\n",
    "        s,\n",
    "    )\n",
    "\n",
    "    # 3) Remove reference-style images used in text: ![alt][id]\n",
    "    s = re.sub(r\"!\\[[^\\]]*\\]\\[[^\\]]*\\]\", \"\", s)\n",
    "\n",
    "    # 4) Remove normal reference definitions (single-line), including <...> urls.\n",
    "    #    Example: [img1]: <https://...> \"title\"\n",
    "    s = re.sub(\n",
    "        r\"(?im)^\\s*\\[[^\\]]+\\]:\\s*(?:<[^>\\n]+>|[^\\s]+)(?:\\s+(?:\\\"[^\\\"]*\\\"|'[^']*'|\\([^)]+\\)))?\\s*$\",\n",
    "        \"\",\n",
    "        s,\n",
    "    )\n",
    "\n",
    "    # 5) Hard-cut from the first 'data:image' reference definition to EOF, in case it's huge / no blank line.\n",
    "    m = re.search(r\"(?is)^\\s*\\[[^\\]]+\\]:\\s*<?\\s*data:image[^>\\s]*.*$\", s, flags=re.MULTILINE)\n",
    "    if m:\n",
    "        s = s[:m.start()]\n",
    "\n",
    "    return _norm(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9b492b26-583d-4f73-8ddc-9195bf5d4a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) map each extension to its extractor\n",
    "EXT_MAP = {\n",
    "    \".docx\": extract_from_docx,\n",
    "    \".pdf\":  extract_from_pdf,\n",
    "    \".epub\": extract_from_zip,    # epub is a ZIP of XHTML/HTML\n",
    "    \".zip\":  extract_from_zip,\n",
    "    \".html\": extract_from_html,\n",
    "    \".odt\":  extract_from_odt,\n",
    "    \".rtf\":  extract_from_rtf,\n",
    "    \".md\":   extract_from_md,\n",
    "}\n",
    "\n",
    "def convert_and_replace(s3, bucket, key, body, dest_bucket=None, keep_original=False):\n",
    "    \"\"\"\n",
    "    Convert supported doc to .txt and upload. Optionally delete the original.\n",
    "    dest_bucket: if None, write back to the same bucket.\n",
    "    keep_original: if True, do not delete the original file.\n",
    "    \"\"\"\n",
    "    ext = os.path.splitext(key)[1].lower()\n",
    "    extractor = EXT_MAP.get(ext)\n",
    "    if extractor is None:\n",
    "        return False  # not supported; caller can skip\n",
    "\n",
    "    print(f\"Converting {key} to txt\")\n",
    "    content = extractor(body)  # run the right extractor\n",
    "    name, _ = os.path.splitext(key)\n",
    "    new_key = name + \".txt\"\n",
    "\n",
    "    target_bucket = dest_bucket or bucket\n",
    "    s3.put_object(\n",
    "        Bucket=target_bucket,\n",
    "        Key=new_key,\n",
    "        Body=content.encode(\"utf-8\"),\n",
    "        ContentType=\"text/plain\",\n",
    "    )\n",
    "    print(f\"Successfully converted {key} to {new_key}.\")\n",
    "\n",
    "    if not keep_original and dest_bucket is None:\n",
    "        # only delete if we stayed in the same bucket; if moving across buckets,\n",
    "        # you can also delete here after confirming the put succeeded.\n",
    "        s3.delete_object(Bucket=bucket, Key=key)\n",
    "\n",
    "    return True\n",
    "\n",
    "def convert_texts_to_txt(bucket, prefix=\"\"):\n",
    "    paginator = s3.get_paginator(\"list_objects_v2\")\n",
    "    for page in paginator.paginate(Bucket=bucket, Prefix=prefix):\n",
    "        for obj in page.get(\"Contents\", []):\n",
    "\n",
    "            if obj['Size'] == 0:  # Skip the folder itself (if the file size is 0)\n",
    "                print(obj)\n",
    "                continue\n",
    "\n",
    "            key = obj[\"Key\"]\n",
    "            resp = s3.get_object(Bucket=bucket, Key=key)\n",
    "            body = resp[\"Body\"].read()  # Read the file content\n",
    "            \n",
    "\n",
    "            if key.endswith(\".txt\"):\n",
    "                try:\n",
    "                    # Get the file object from S3\n",
    "    \n",
    "                    # Use chardet to detect the file encoding\n",
    "                    result = chardet.detect(body)\n",
    "                    current_encoding = result['encoding']\n",
    "                    # Skip if the file is already in UTF-8 encoding\n",
    "                    if (current_encoding == \"utf-8\" or current_encoding == \"ascii\"):\n",
    "                        continue\n",
    "                    print(current_encoding)\n",
    "                    print(f\"Converting {key} from {current_encoding} to UTF-8\")\n",
    "\n",
    "                    # Decode the content using the detected encoding and re-encode it in UTF-8\n",
    "                    content = body.decode(current_encoding, errors='ignore')  # Ignore characters that can't be decoded\n",
    "                    \n",
    "                    # Upload the converted content back to S3 in UTF-8\n",
    "                    s3.put_object(\n",
    "                        Bucket=bucket,\n",
    "                        Key=key,  # Make sure the file key (path) is correct\n",
    "                        Body=content.encode('utf-8'),\n",
    "                        ContentType=\"text/plain\"\n",
    "                    )\n",
    "                    print(f\"Successfully converted {key} to UTF-8.\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to process {key}: {e}\")  # Print error if something goes wrong\n",
    "            else:\n",
    "                handled = convert_and_replace(s3, bucket, key, body)  # or dest_bucket=\"trusted_zone\"\n",
    "                if not handled:\n",
    "                    print(f\"Skip (unsupported ext): {key}\")\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6895ffcf-6008-4eaa-a469-c695d07aa0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting texts/Zheshuo Linmd.md to txt\n",
      "Successfully converted texts/Zheshuo Linmd.md to texts/Zheshuo Linmd.txt.\n"
     ]
    }
   ],
   "source": [
    "convert_texts_to_txt(bucket = \"formatted-zone\", prefix = \"texts/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edae4770-2abf-4ca0-8380-7378c397bdb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d818298e-c606-4d72-ae36-9e269becf8c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
